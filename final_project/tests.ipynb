{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL for the 24hr ticker price change statistics endpoint\n",
    "url = \"https://api.binance.com/api/v3/ticker/24hr\"\n",
    "\n",
    "# Make the request to get the ticker information\n",
    "response = requests.get(url)\n",
    "ticker_data = response.json()\n",
    "\n",
    "# Print trading volume for each market\n",
    "print(\"24hr Trading Volumes:\")\n",
    "for ticker in ticker_data:\n",
    "    symbol = ticker['symbol']\n",
    "    volume = ticker['volume']\n",
    "    quote_volume = ticker['quoteVolume']\n",
    "    print(f\"Market: {symbol}, Volume: {volume}, Quote Volume: {quote_volume}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL for the Binance exchange information endpoint\n",
    "url = \"https://api.binance.com/api/v3/exchangeInfo\"\n",
    "\n",
    "# Make the request to get the exchange info\n",
    "response = requests.get(url)\n",
    "exchange_info = response.json()\n",
    "\n",
    "# Extract unique quote assets\n",
    "quote_assets = set()\n",
    "for symbol in exchange_info['symbols']:\n",
    "    quote_assets.add(symbol['quoteAsset'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URLs\n",
    "exchange_info_url = \"https://api.binance.com/api/v3/exchangeInfo\"\n",
    "ticker_24hr_url = \"https://api.binance.com/api/v3/ticker/24hr\"\n",
    "\n",
    "# Send requests to both URLs\n",
    "exchange_info_response = requests.get(exchange_info_url)\n",
    "ticker_24hr_response = requests.get(ticker_24hr_url)\n",
    "\n",
    "# Parse the JSON responses\n",
    "exchange_info = exchange_info_response.json()\n",
    "ticker_24hr_info = ticker_24hr_response.json()\n",
    "\n",
    "# Create a dictionary to map symbols to their base and quote assets\n",
    "symbol_info_map = {}\n",
    "for symbol_info in exchange_info['symbols']:\n",
    "    symbol = symbol_info['symbol']\n",
    "    base_asset = symbol_info['baseAsset']\n",
    "    quote_asset = symbol_info['quoteAsset']\n",
    "    symbol_info_map[symbol] = {\n",
    "        'symbol': symbol,\n",
    "        'baseAsset': base_asset,\n",
    "        'quoteAsset': quote_asset\n",
    "    }\n",
    "\n",
    "# Create the final list of dictionaries with the required keys\n",
    "result = []\n",
    "for ticker in ticker_24hr_info:\n",
    "    symbol = ticker['symbol']\n",
    "    if symbol in symbol_info_map:\n",
    "        combined_info = symbol_info_map[symbol]\n",
    "        combined_info['priceChangePercent'] = ticker['priceChangePercent']\n",
    "        combined_info['quoteVolume'] = ticker['quoteVolume']\n",
    "        result.append(combined_info)\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.data_handler as m \n",
    "\n",
    "handler = m.DataHandler(min_volume=10e6)\n",
    "\n",
    "result = handler.get_trading_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in result:\n",
    "    print(\"------------------------------\")\n",
    "    print(pair)\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in result:\n",
    "    print(\"------------------------------\")\n",
    "    print(pair)\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "async def fetch(session, url):\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            if response.status == 200:\n",
    "                return await response.json()\n",
    "            else:\n",
    "                return {'error': f\"Failed to fetch {url}: {response.status}\"}\n",
    "    except aiohttp.ClientError as e:\n",
    "        return {'error': f\"Request failed for {url}: {str(e)}\"}\n",
    "\n",
    "async def fetch_all(urls):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch(session, url) for url in urls]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "def process_responses(responses):\n",
    "    responses_list = []\n",
    "    for response in responses:\n",
    "        if 'error' in response:\n",
    "            print(response['error'])\n",
    "        else:\n",
    "            responses_list.append(response)\n",
    "    return responses_list\n",
    "\n",
    "urls = [\n",
    "    'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/1m/BTCUSDT-1m-2024-05.zip',\n",
    "    'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/1m/BTCUSDT-1m-2024-04.zip',\n",
    "    'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/1m/BTCUSDT-1m-2024-03.zip',\n",
    "    'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/1m/BTCUSDT-1m-2024-02.zip',\n",
    "    'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/1m/BTCUSDT-1m-2024-01.zip'\n",
    "]\n",
    "\n",
    "async def main():\n",
    "    responses = await fetch_all(urls)\n",
    "    return process_responses(responses)\n",
    "\n",
    "# Run the asyncio event loop\n",
    "l = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "x = os.listdir('data/downloads')\n",
    "\n",
    "unique = []\n",
    "\n",
    "for file in x: \n",
    "    p = file.split('-')[0]\n",
    "    unique.append(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "import vectorbt as vbt\n",
    "\n",
    "# Step 1: Read the Parquet file\n",
    "df = pd.read_parquet('data/ETHUSDT/ETHUSDT-1m-2018-01.parquet')\n",
    "\n",
    "# Ensure the column names are in lower case to match your data labels\n",
    "df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "\n",
    "# Set open_time as the DataFrame index\n",
    "df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "df.set_index('open_time', inplace=True)\n",
    "\n",
    "# Step 2: Calculate Ichimoku Cloud components\n",
    "# Calculate Ichimoku Cloud components\n",
    "ichimoku_cloud, _ = df.ta.ichimoku(tenkan_sen=True, kijun_sen=True, senkou_span=True, chikou_span=False)\n",
    "\n",
    "# Extract individual components\n",
    "tenkan_sen = ichimoku_cloud['ITS_9']\n",
    "kijun_sen = ichimoku_cloud['IKS_26']\n",
    "span_a = ichimoku_cloud['ISA_9']\n",
    "span_b = ichimoku_cloud['ISB_26']\n",
    "\n",
    "# Step 3: Define Strategy Logic\n",
    "# Strategy rules: Long when close is above both span A and span B, and tenkan_sen is above kijun_sen\n",
    "long_entries = (df['close'] > span_a) & (df['close'] > span_b) & (tenkan_sen > kijun_sen)\n",
    "# Short when close is below both span A and span B, and tenkan_sen is below kijun_sen\n",
    "short_entries = (df['close'] < span_a) & (df['close'] < span_b) & (tenkan_sen < kijun_sen)\n",
    "\n",
    "# Step 4: Backtest the Strategy with vectorbt\n",
    "# Create signals DataFrame\n",
    "signals = pd.DataFrame(index=df.index)\n",
    "signals['long'] = long_entries.astype(int)\n",
    "signals['short'] = short_entries.astype(int)\n",
    "\n",
    "# Create portfolio\n",
    "portfolio = vbt.Portfolio.from_signals(\n",
    "    close=df['close'],\n",
    "    entries=signals['long'],\n",
    "    exits=signals['short'],\n",
    "    freq='1m'  # Daily frequency for closing positions\n",
    ")\n",
    "\n",
    "# Calculate portfolio statistics\n",
    "stats = portfolio.stats()\n",
    "\n",
    "# Step 5: Plotting the Backtest Results\n",
    "# Plot the portfolio performance\n",
    "portfolio.plot().show()\n",
    "\n",
    "# Plot entries and exits on price chart\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.plot(df.index, df['close'], label='Close')\n",
    "ax.plot(tenkan_sen.index, tenkan_sen, label='Tenkan-sen', linestyle='--')\n",
    "ax.plot(kijun_sen.index, kijun_sen, label='Kijun-sen', linestyle='--')\n",
    "ax.plot(span_a.index, span_a, label='Span A', linestyle='--')\n",
    "ax.plot(span_b.index, span_b, label='Span B', linestyle='--')\n",
    "\n",
    "# Plot entry and exit signals\n",
    "ax.plot(signals[signals['long'] == 1].index, df.loc[signals['long'] == 1, 'close'], '^', markersize=10, color='g', label='Long Entry')\n",
    "ax.plot(signals[signals['short'] == 1].index, df.loc[signals['short'] == 1, 'close'], 'v', markersize=10, color='r', label='Short Entry')\n",
    "\n",
    "ax.legend()\n",
    "plt.title('Ichimoku Cloud Strategy')\n",
    "plt.show()\n",
    "\n",
    "# Print portfolio statistics\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/ETHUSDT/ETHUSDT-1m-2018-01.parquet')\n",
    "\n",
    "# Ensure the column names are in lower case to match your data labels\n",
    "df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "\n",
    "# Set open_time as the DataFrame index\n",
    "df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "df.set_index('open_time', inplace=True)\n",
    "\n",
    "# Step 2: Calculate Ichimoku Cloud components\n",
    "# Calculate Ichimoku Cloud components\n",
    "ichimoku_cloud, _ = df.ta.ichimoku(tenkan_sen=True, kijun_sen=True, senkou_span=True, chikou_span=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ichimoku_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "import vectorbt as vbt\n",
    "\n",
    "# Step 1: Read the Parquet file\n",
    "df = pd.read_parquet('data/ETHUSDT/ETHUSDT-1m-2018-01.parquet')\n",
    "\n",
    "df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "\n",
    "# Resample to 5-minute intervals\n",
    "\n",
    "df_resampled = df.resample('1min', on='open_time').agg({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last'\n",
    "})\n",
    "\n",
    "\n",
    "# Ensure all columns are properly aggregated\n",
    "df = df_resampled.reset_index(inplace=False)\n",
    "\n",
    "\n",
    "# Ensure the column names are in lower case to match your data labels\n",
    "df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "\n",
    "# Step 2: Calculate Ichimoku Cloud components\n",
    "# Calculate Ichimoku Cloud components\n",
    "ichimoku_cloud, _ = df.ta.ichimoku(tenkan_sen=True, kijun_sen=True, senkou_span=True, chikou_span=False)\n",
    "\n",
    "# Extract individual components\n",
    "tenkan_sen = ichimoku_cloud['ITS_9']\n",
    "kijun_sen = ichimoku_cloud['IKS_26']\n",
    "span_a = ichimoku_cloud['ISA_9']\n",
    "span_b = ichimoku_cloud['ISB_26']\n",
    "\n",
    "# Step 3: Define Strategy Logic\n",
    "# Strategy rules: Long when close is above both span A and span B, and tenkan_sen is above kijun_sen\n",
    "long_entries = (df['close'] > span_a) & (df['close'] > span_b) & (tenkan_sen > kijun_sen)\n",
    "# Short when close is below both span A and span B, and tenkan_sen is below kijun_sen\n",
    "short_entries = (df['close'] < span_a) & (df['close'] < span_b) & (tenkan_sen < kijun_sen)\n",
    "\n",
    "# Exit conditions for long positions\n",
    "long_exits = ~(tenkan_sen > kijun_sen) \n",
    "\n",
    "# Exit conditions for short positions\n",
    "short_exits = ~(tenkan_sen < kijun_sen) \n",
    "\n",
    "# Step 4: Backtest the Strategy with vectorbt\n",
    "# Create signals DataFrame\n",
    "signals = pd.DataFrame(index=df.index)\n",
    "signals['long'] = long_entries.astype(int)\n",
    "signals['short'] = short_entries.astype(int)\n",
    "\n",
    "# Create exit signals\n",
    "signals['long_exit'] = long_exits.astype(int)\n",
    "signals['short_exit'] = short_exits.astype(int)\n",
    "\n",
    "# Create portfolio\n",
    "portfolio = vbt.Portfolio.from_signals(\n",
    "    close=df['close'],\n",
    "    entries=signals['long'],\n",
    "    exits=signals['long_exit'],\n",
    "    short_entries=signals['short'],\n",
    "    short_exits=signals['short_exit'],\n",
    "    freq='1m'  # Daily frequency for closing positions\n",
    ")\n",
    "\n",
    "# Calculate portfolio statistics\n",
    "stats = portfolio.stats()\n",
    "\n",
    "# Step 5: Plotting the Backtest Results\n",
    "# Plot the portfolio performance\n",
    "portfolio.plot().show()\n",
    "\n",
    "# Plot entries and exits on price chart\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.plot(df.index, df['close'], label='Close')\n",
    "ax.plot(tenkan_sen.index, tenkan_sen, label='Tenkan-sen', linestyle='--')\n",
    "ax.plot(kijun_sen.index, kijun_sen, label='Kijun-sen', linestyle='--')\n",
    "ax.plot(span_a.index, span_a, label='Span A', linestyle='--')\n",
    "ax.plot(span_b.index, span_b, label='Span B', linestyle='--')\n",
    "\n",
    "# Plot entry signals\n",
    "ax.plot(signals[signals['long'] == 1].index, df.loc[signals['long'] == 1, 'close'], '^', markersize=10, color='g', label='Long Entry')\n",
    "ax.plot(signals[signals['short'] == 1].index, df.loc[signals['short'] == 1, 'close'], 'v', markersize=10, color='r', label='Short Entry')\n",
    "\n",
    "# Plot exit signals\n",
    "ax.plot(signals[signals['long_exit'] == 1].index, df.loc[signals['long_exit'] == 1, 'close'], 'o', markersize=7, color='g', label='Long Exit')\n",
    "ax.plot(signals[signals['short_exit'] == 1].index, df.loc[signals['short_exit'] == 1, 'close'], 'o', markersize=7, color='r', label='Short Exit')\n",
    "\n",
    "ax.legend()\n",
    "plt.title('Ichimoku Cloud Strategy with Exit Signals')\n",
    "plt.show()\n",
    "\n",
    "# Print portfolio statistics\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "import vectorbt as vbt\n",
    "\n",
    "# Step 1: Read the Parquet file\n",
    "df = pd.read_parquet('data/ETHUSDT/ETHUSDT-1m-2023-09.parquet')\n",
    "\n",
    "# Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "\n",
    "# Resample to 5-minute intervals\n",
    "df_resampled = df.resample('1min', on='open_time').agg({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last'\n",
    "})\n",
    "\n",
    "# Ensure all columns are properly aggregated\n",
    "df = df_resampled.reset_index()\n",
    "\n",
    "# Ensure the column names are in lower case to match your data labels\n",
    "df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "\n",
    "# Step 2: Calculate Ichimoku Cloud components\n",
    "# Calculate Ichimoku Cloud components\n",
    "ichimoku_cloud, _ = df.ta.ichimoku(tenkan_sen=True, kijun_sen=True, senkou_span=True, chikou_span=False)\n",
    "\n",
    "# Extract individual components\n",
    "tenkan_sen = ichimoku_cloud['ITS_9']\n",
    "kijun_sen = ichimoku_cloud['IKS_26']\n",
    "span_a = ichimoku_cloud['ISA_9']\n",
    "span_b = ichimoku_cloud['ISB_26']\n",
    "\n",
    "# Step 3: Define Strategy Logic\n",
    "\n",
    "# Strategy rules: Long when close is above both span A and span B, tenkan_sen is above kijun_sen, and RSI < 30\n",
    "long_entries = (df['close'] > span_a) & (df['close'] > span_b) & (tenkan_sen > kijun_sen) \n",
    "# Short when close is below both span A and span B, tenkan_sen is below kijun_sen, and RSI > 70\n",
    "short_entries = (df['close'] < span_a) & (df['close'] < span_b) & (tenkan_sen < kijun_sen)\n",
    "\n",
    "# Exit conditions for long positions: tenkan_sen < kijun_sen or RSI > 70\n",
    "long_exits = (tenkan_sen < kijun_sen) \n",
    "# Exit conditions for short positions: tenkan_sen > kijun_sen or RSI < 30\n",
    "short_exits = (tenkan_sen > kijun_sen) \n",
    "\n",
    "# Step 4: Backtest the Strategy with vectorbt\n",
    "# Create signals DataFrame\n",
    "signals = pd.DataFrame(index=df.index)\n",
    "signals['long'] = long_entries.astype(int)\n",
    "signals['short'] = short_entries.astype(int)\n",
    "\n",
    "# Create exit signals\n",
    "signals['long_exit'] = long_exits.astype(int)\n",
    "signals['short_exit'] = short_exits.astype(int)\n",
    "\n",
    "# Create portfolio\n",
    "portfolio = vbt.Portfolio.from_signals(\n",
    "    close=df['close'],\n",
    "    entries=signals['long'],\n",
    "    exits=signals['long_exit'],\n",
    "    short_entries=signals['short'],\n",
    "    short_exits=signals['short_exit'],\n",
    "    freq='1m'  # 5-minute frequency for closing positions\n",
    ")\n",
    "\n",
    "# Calculate portfolio statistics\n",
    "stats = portfolio.stats()\n",
    "\n",
    "# Step 5: Plotting the Backtest Results\n",
    "# Plot the portfolio performance\n",
    "portfolio.plot().show()\n",
    "\n",
    "# Plot entries and exits on price chart\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.plot(df['open_time'], df['close'], label='Close')\n",
    "ax.plot(tenkan_sen.index, tenkan_sen, label='Tenkan-sen', linestyle='--')\n",
    "ax.plot(kijun_sen.index, kijun_sen, label='Kijun-sen', linestyle='--')\n",
    "ax.plot(span_a.index, span_a, label='Span A', linestyle='--')\n",
    "ax.plot(span_b.index, span_b, label='Span B', linestyle='--')\n",
    "\n",
    "# Plot entry signals\n",
    "ax.plot(signals[signals['long'] == 1].index, df.loc[signals['long'] == 1, 'close'], '^', markersize=10, color='g', label='Long Entry')\n",
    "ax.plot(signals[signals['short'] == 1].index, df.loc[signals['short'] == 1, 'close'], 'v', markersize=10, color='r', label='Short Entry')\n",
    "\n",
    "# Plot exit signals\n",
    "ax.plot(signals[signals['long_exit'] == 1].index, df.loc[signals['long_exit'] == 1, 'close'], 'o', markersize=7, color='g', label='Long Exit')\n",
    "ax.plot(signals[signals['short_exit'] == 1].index, df.loc[signals['short_exit'] == 1, 'close'], 'o', markersize=7, color='r', label='Short Exit')\n",
    "\n",
    "ax.legend()\n",
    "plt.title('Ichimoku Cloud Strategy with RSI Conditions')\n",
    "plt.show()\n",
    "\n",
    "# Print portfolio statistics\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "import vectorbt as vbt\n",
    "import os \n",
    "\n",
    "\n",
    "def backtest(data):\n",
    "    # Step 1: Read the Parquet file\n",
    "    df = data\n",
    "\n",
    "    # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    \n",
    "    # Resample to 5-minute intervals\n",
    "    df_resampled = df.resample('1min', on='open_time').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    })\n",
    "    \n",
    "    # Ensure all columns are properly aggregated\n",
    "    df = df_resampled.reset_index()\n",
    "    \n",
    "    # Ensure the column names are in lower case to match your data labels\n",
    "    df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "    \n",
    "    # Step 2: Calculate Ichimoku Cloud components\n",
    "    # Calculate Ichimoku Cloud components\n",
    "    ichimoku_cloud, _ = df.ta.ichimoku(tenkan_sen=True, kijun_sen=True, senkou_span=True, chikou_span=False)\n",
    "    \n",
    "    # Extract individual components\n",
    "    tenkan_sen = ichimoku_cloud['ITS_9']\n",
    "    kijun_sen = ichimoku_cloud['IKS_26']\n",
    "    span_a = ichimoku_cloud['ISA_9']\n",
    "    span_b = ichimoku_cloud['ISB_26']\n",
    "    \n",
    "    # Step 3: Define Strategy Logic\n",
    "    # Calculate RSI\n",
    "    df['rsi'] = ta.rsi(df['close'], length=14)\n",
    "    \n",
    "    # Define Strategy Logic\n",
    "    # Long when close is above both span A and span B, tenkan_sen is above kijun_sen, and RSI < 30\n",
    "    long_entries = (df['close'] > span_a) & (df['close'] > span_b) & (tenkan_sen > kijun_sen)\n",
    "    # Short when close is below both span A and span B, tenkan_sen is below kijun_sen, and RSI > 70\n",
    "    short_entries = (df['close'] < span_a) & (df['close'] < span_b) & (tenkan_sen < kijun_sen)\n",
    "    \n",
    "    # Exit conditions for long positions: tenkan_sen < kijun_sen or RSI > 70\n",
    "    long_exits = (tenkan_sen < kijun_sen) | (df['rsi'] > 70)\n",
    "    # Exit conditions for short positions: tenkan_sen > kijun_sen or RSI < 30\n",
    "    short_exits = (tenkan_sen > kijun_sen) | (df['rsi'] < 30) \n",
    "    \n",
    "    # Step 4: Backtest the Strategy with vectorbt\n",
    "    # Create signals DataFrame\n",
    "    signals = pd.DataFrame(index=df.index)\n",
    "    signals['long'] = long_entries.astype(int)\n",
    "    signals['short'] = short_entries.astype(int)\n",
    "    \n",
    "    # Create exit signals\n",
    "    signals['long_exit'] = long_exits.astype(int)\n",
    "    signals['short_exit'] = short_exits.astype(int)\n",
    "    \n",
    "    # Create portfolio\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        close=df['close'],\n",
    "        entries=signals['long'],\n",
    "        exits=signals['long_exit'],\n",
    "        short_entries=signals['short'],\n",
    "        short_exits=signals['short_exit'],\n",
    "        freq='1m',  # 5-minute frequency for closing positions\n",
    "    )\n",
    "    \n",
    "    # Calculate portfolio statistics\n",
    "    stats = portfolio.stats()\n",
    "    \n",
    "    # Step 5: Plotting the Backtest Results\n",
    "    # Plot the portfolio performance\n",
    "    portfolio.plot().show()\n",
    "    # Print portfolio statistics\n",
    "    print(stats)\n",
    "\n",
    "def concat_parquet_files(file_paths):\n",
    "    dfs = []\n",
    "    \n",
    "    # Iterate through each file path\n",
    "    for file_path in file_paths:\n",
    "        # Read the parquet file\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "        df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames in the list along rows\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return concatenated_df\n",
    "\n",
    "count=0\n",
    "paths = []\n",
    "for file in os.listdir('data/DOGEUSDT'):\n",
    "    if file.endswith('.parquet'):\n",
    "        paths.append(os.path.join('data/DOGEUSDT', file))\n",
    "\n",
    "\n",
    "eth_data = concat_parquet_files(paths)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "import vectorbt as vbt\n",
    "import os \n",
    "import src.utils as ut\n",
    "\n",
    "def backtest(file):\n",
    "    # Step 1: Read the Parquet file\n",
    "    df = pd.read_parquet(file)\n",
    "\n",
    "    # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    \n",
    "    # Resample to 1-minute intervals (adjust if needed)\n",
    "    df_resampled = df.resample('15min', on='open_time').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    })\n",
    "    \n",
    "    # Ensure all columns are properly aggregated\n",
    "    df = df_resampled.reset_index()\n",
    "    \n",
    "    # Ensure the column names are in lower case to match your data labels\n",
    "    df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "    \n",
    "    # Step 2: Calculate Ichimoku Cloud components\n",
    "    # Calculate Ichimoku Cloud components\n",
    "    ichimoku_cloud, _ = df.ta.ichimoku(tenkan_sen=True, kijun_sen=True, senkou_span=True, chikou_span=False)\n",
    "    \n",
    "    # Extract individual components\n",
    "    tenkan_sen = ichimoku_cloud['ITS_9']\n",
    "    kijun_sen = ichimoku_cloud['IKS_26']\n",
    "    span_a = ichimoku_cloud['ISA_9']\n",
    "    span_b = ichimoku_cloud['ISB_26']\n",
    "    \n",
    "    # Step 3: Calculate RSI\n",
    "    df['rsi'] = ta.rsi(df['close'], length=14)\n",
    "    \n",
    "    # Step 4: Define Strategy Logic\n",
    "    # Create signals DataFrame\n",
    "    signals = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Define Strategy Logic\n",
    "    # Long when tenkan_sen crosses over kijun_sen, close is above both span A and span B, and RSI < 30\n",
    "    long_entries = (ut.crossed_over(tenkan_sen, kijun_sen)) & (df['close'] > span_a) & (df['close'] > span_b) \n",
    "    \n",
    "    # Short when tenkan_sen crosses below kijun_sen, close is below both span A and span B, and RSI > 70\n",
    "    short_entries = (ut.crossed_below(tenkan_sen, kijun_sen)) & (df['close'] < span_a) & (df['close'] < span_b) \n",
    "    \n",
    "    # Exit conditions for long positions: tenkan_sen < kijun_sen or RSI > 70\n",
    "    long_exits = ut.crossed_below(tenkan_sen, kijun_sen)\n",
    "    # Exit conditions for short positions: tenkan_sen > kijun_sen or RSI < 30\n",
    "    short_exits = ut.crossed_over(tenkan_sen, kijun_sen) \n",
    "    \n",
    "    # Assign strategy signals to the signals DataFrame\n",
    "    signals['long'] = long_entries.astype(int)\n",
    "    signals['short'] = short_entries.astype(int)\n",
    "    signals['long_exit'] = long_exits.astype(int)\n",
    "    signals['short_exit'] = short_exits.astype(int)\n",
    "    \n",
    "    # Step 5: Backtest the Strategy with vectorbt\n",
    "    # Create portfolio\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        close=df['close'],\n",
    "        entries=signals['long'],\n",
    "        exits=signals['long_exit'],\n",
    "        short_entries=signals['short'],\n",
    "        short_exits=signals['short_exit'],\n",
    "        freq='1min',  # Frequency for closing positions (adjust as needed)\n",
    "        fees=0.001\n",
    "    )\n",
    "    \n",
    "    # Calculate portfolio statistics\n",
    "    stats = portfolio.stats()\n",
    "    \n",
    "    # Step 6: Plotting the Backtest Results\n",
    "    # Plot the portfolio performance\n",
    "    portfolio.plot().show()\n",
    "    # Print portfolio statistics\n",
    "    print(stats)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def backtest_data(data):\n",
    "    # Step 1: Read the Parquet file\n",
    "    df = data\n",
    "\n",
    "    # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    \n",
    "    # Resample to 1-minute intervals (adjust if needed)\n",
    "    df_resampled = df.resample('15min', on='open_time').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    })\n",
    "    \n",
    "    # Ensure all columns are properly aggregated\n",
    "    df = df_resampled.reset_index()\n",
    "    \n",
    "    # Ensure the column names are in lower case to match your data labels\n",
    "    df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "    \n",
    "    # Step 2: Calculate Ichimoku Cloud components\n",
    "    # Calculate Ichimoku Cloud components\n",
    "    ichimoku_cloud, _ = df.ta.ichimoku(tenkan_sen=True, kijun_sen=True, senkou_span=True, chikou_span=False)\n",
    "    \n",
    "    # Extract individual components\n",
    "    tenkan_sen = ichimoku_cloud['ITS_9']\n",
    "    kijun_sen = ichimoku_cloud['IKS_26']\n",
    "    span_a = ichimoku_cloud['ISA_9']\n",
    "    span_b = ichimoku_cloud['ISB_26']\n",
    "    \n",
    "    # Step 3: Calculate RSI\n",
    "    df['rsi'] = ta.rsi(df['close'], length=14)\n",
    "    \n",
    "    # Step 4: Define Strategy Logic\n",
    "    # Create signals DataFrame\n",
    "    signals = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Define Strategy Logic\n",
    "    # Long when tenkan_sen crosses over kijun_sen, close is above both span A and span B, and RSI < 30\n",
    "    long_entries = (ut.crossed_over(tenkan_sen, kijun_sen)) & (df['close'] > span_a) & (df['close'] > span_b) \n",
    "    \n",
    "    # Short when tenkan_sen crosses below kijun_sen, close is below both span A and span B, and RSI > 70\n",
    "    short_entries = (ut.crossed_below(tenkan_sen, kijun_sen)) & (df['close'] < span_a) & (df['close'] < span_b) \n",
    "    \n",
    "    # Exit conditions for long positions: tenkan_sen < kijun_sen or RSI > 70\n",
    "    long_exits = ut.crossed_below(tenkan_sen, kijun_sen)\n",
    "    # Exit conditions for short positions: tenkan_sen > kijun_sen or RSI < 30\n",
    "    short_exits = ut.crossed_over(tenkan_sen, kijun_sen) \n",
    "    \n",
    "    # Assign strategy signals to the signals DataFrame\n",
    "    signals['long'] = long_entries.astype(int)\n",
    "    signals['short'] = short_entries.astype(int)\n",
    "    signals['long_exit'] = long_exits.astype(int)\n",
    "    signals['short_exit'] = short_exits.astype(int)\n",
    "    \n",
    "    # Step 5: Backtest the Strategy with vectorbt\n",
    "    # Create portfolio\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        close=df['close'],\n",
    "        entries=signals['long'],\n",
    "        exits=signals['long_exit'],\n",
    "        short_entries=signals['short'],\n",
    "        short_exits=signals['short_exit'],\n",
    "        freq='1min',  # Frequency for closing positions (adjust as needed)\n",
    "        fees=0.001\n",
    "    )\n",
    "    \n",
    "    # Calculate portfolio statistics\n",
    "    stats = portfolio.stats()\n",
    "    \n",
    "    # Step 6: Plotting the Backtest Results\n",
    "    # Plot the portfolio performance\n",
    "    portfolio.plot().show()\n",
    "    # Print portfolio statistics\n",
    "    print(stats)\n",
    "\n",
    "def concat_parquet_files(file_paths):\n",
    "    dfs = []\n",
    "    \n",
    "    # Iterate through each file path\n",
    "    for file_path in file_paths:\n",
    "        # Read the parquet file\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "        df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames in the list along rows\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return concatenated_df\n",
    "\n",
    "paths = []\n",
    "for file in os.listdir('data/DOGEUSDT'):\n",
    "    if file.endswith('.parquet'):\n",
    "        paths.append(os.path.join('data/DOGEUSDT', file))\n",
    "\n",
    "d = concat_parquet_files(paths)\n",
    "\n",
    "backtest_data(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "####### OPTIMIZATION #########\n",
    "##############################\n",
    "\n",
    "import pandas as pd \n",
    "import pandas_ta as ta \n",
    "import vectorbt as vbt\n",
    "import numpy as np\n",
    "from src.utils import crossed_below, crossed_over\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def backtest_strategy(data, tenkan_sen_length, kijun_sen_length, senkou_span_length):\n",
    "    # Step 1: Read the Parquet file\n",
    "    df = data.copy()\n",
    "\n",
    "    # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    \n",
    "    # Resample to 15-minute intervals (adjust if needed)\n",
    "    df_resampled = df.resample('15min', on='open_time').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    })\n",
    "    \n",
    "    # Ensure all columns are properly aggregated\n",
    "    df = df_resampled.reset_index()\n",
    "    \n",
    "    # Ensure the column names are in lower case to match your data labels\n",
    "    df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "    \n",
    "    # Step 2: Calculate Ichimoku Cloud components\n",
    "    # Calculate Ichimoku Cloud components with specified parameters\n",
    "    ichimoku_cloud, _ = ta.ichimoku(high= df['high'], low=df['low'], close=df['close'], tenkan=tenkan_sen_length, kijun=kijun_sen_length, senkou=senkou_span_length, include_chikou=False)\n",
    "\n",
    "    # ichimoku_cloud.rename(columns={'ISA_9': f'ISA_{senkou_span_length//2}',\n",
    "    #                                 'ISB_26':f'ISB_{senkou_span_length}',\n",
    "    #                                 'ITS_9':f'ITS_{tenkan_sen_length}',\n",
    "    #                                 'IKS_26':f'IKS_{kijun_sen_length}' }, inplace=True)\n",
    "    # ichimoku_cloud.drop(columns=['ICS_26'])\n",
    "    \n",
    "    # Extract individual components\n",
    "    tenkan_sen = ichimoku_cloud[f'ITS_{tenkan_sen_length}']\n",
    "    kijun_sen = ichimoku_cloud[f'IKS_{kijun_sen_length}']\n",
    "    span_a = ichimoku_cloud[f'ISA_{tenkan_sen_length}']  # Adjust for span A\n",
    "    span_b = ichimoku_cloud[f'ISB_{kijun_sen_length}']\n",
    "    \n",
    "    # Step 3: Define Strategy Logic\n",
    "    # Create signals DataFrame\n",
    "    signals = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Define Strategy Logic based on extracted components\n",
    "    # Long when tenkan_sen crosses over kijun_sen, close is above both span A and span B\n",
    "    long_entries = crossed_over(tenkan_sen, kijun_sen) & (df['close'] > span_a) & (df['close'] > span_b)\n",
    "    \n",
    "    # Short when tenkan_sen crosses below kijun_sen, close is below both span A and span B\n",
    "    short_entries = crossed_below(tenkan_sen, kijun_sen) & (df['close'] < span_a) & (df['close'] < span_b) \n",
    "    \n",
    "    # Exit conditions for long positions: tenkan_sen < kijun_sen\n",
    "    long_exits = crossed_below(tenkan_sen, kijun_sen)\n",
    "    \n",
    "    # Exit conditions for short positions: tenkan_sen > kijun_sen\n",
    "    short_exits = crossed_over(tenkan_sen, kijun_sen)\n",
    "    \n",
    "    # Assign strategy signals to the signals DataFrame\n",
    "    signals['long'] = long_entries.astype(int)\n",
    "    signals['short'] = short_entries.astype(int)\n",
    "    signals['long_exit'] = long_exits.astype(int)\n",
    "    signals['short_exit'] = short_exits.astype(int)\n",
    "    \n",
    "    # Step 4: Backtest the Strategy with vectorbt\n",
    "    # Create portfolio\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        close=df['close'],\n",
    "        entries=signals['long'],\n",
    "        exits=signals['long_exit'],\n",
    "        short_entries=signals['short'],\n",
    "        short_exits=signals['short_exit'],\n",
    "        freq='15min',  # Frequency for closing positions (adjust as needed)\n",
    "        fees=0.001\n",
    "    )\n",
    "    \n",
    "    # Calculate portfolio statistics\n",
    "    stats = portfolio.stats()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def optimize_strategy(data):\n",
    "    parameter_grid = {\n",
    "        'tenkan_sen_length': range(5, 51, 5),    # Adjust range and step size as needed\n",
    "        'kijun_sen_length': range(20, 61, 5),\n",
    "        'senkou_span_length': range(30, 91, 5)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for tenkan_sen_length in parameter_grid['tenkan_sen_length']:\n",
    "        for kijun_sen_length in parameter_grid['kijun_sen_length']:\n",
    "            for senkou_span_length in parameter_grid['senkou_span_length']:\n",
    "                # Run backtest with current parameter combination\n",
    "                print(f\"Testing Params: {tenkan_sen_length} ,{kijun_sen_length} ,{senkou_span_length}\")\n",
    "                stats = backtest_strategy(data, tenkan_sen_length, kijun_sen_length, senkou_span_length)\n",
    "                \n",
    "                # Store results\n",
    "                results[(tenkan_sen_length, kijun_sen_length, senkou_span_length)] = stats\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def find_best_parameters(results, metric='Sharpe Ratio'):\n",
    "    # Convert results dictionary to DataFrame for easier manipulation\n",
    "    results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "    \n",
    "    # Find parameters that maximize the chosen metric\n",
    "    if metric == 'Sharpe Ratio':\n",
    "        best_params = results_df['Sharpe Ratio'].idxmax()\n",
    "    elif metric == 'Total Return':\n",
    "        best_params = results_df['Total Return'].idxmax()\n",
    "    # Add more metrics as needed\n",
    "    \n",
    "    best_stats = results_df.loc[best_params]\n",
    "    \n",
    "    return best_params, best_stats\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_scatter(results, metric='Sharpe Ratio'):\n",
    "    # Extract parameter values and metric values from results\n",
    "    tenkan_sen_values = [key[0] for key in results.keys()]\n",
    "    kijun_sen_values = [key[1] for key in results.keys()]\n",
    "    senkou_span_values = [key[2] for key in results.keys()]\n",
    "    \n",
    "    # Extract metric values\n",
    "    metric_values = []\n",
    "    for key in results.keys():\n",
    "        stats = results[key]\n",
    "        if metric == 'Sharpe Ratio':\n",
    "            metric_value = stats['Sharpe Ratio']\n",
    "        elif metric == 'Total Return':\n",
    "            metric_value = stats['Total Return']\n",
    "        else:\n",
    "            metric_value = None  # Handle additional metrics if needed\n",
    "        metric_values.append(metric_value)\n",
    "    \n",
    "    # Create scatter plot using Plotly\n",
    "    fig = go.Figure(data=go.Scatter3d(\n",
    "        x=tenkan_sen_values,\n",
    "        y=kijun_sen_values,\n",
    "        z=senkou_span_values,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=12,\n",
    "            color=metric_values,\n",
    "            colorscale='Viridis',  # Choose a colorscale\n",
    "            opacity=0.8,\n",
    "            colorbar=dict(title=metric)\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='Tenkan-sen Length',\n",
    "            yaxis_title='Kijun-sen Length',\n",
    "            zaxis_title='Senkou Span Length'\n",
    "        ),\n",
    "        title=f'Optimization Scatter Plot ({metric})'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "def concat_parquet_files(file_paths):\n",
    "    dfs = []\n",
    "    \n",
    "    # Iterate through each file path\n",
    "    for file_path in file_paths:\n",
    "        # Read the parquet file\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "        df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames in the list along rows\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return concatenated_df\n",
    "\n",
    "print(\"reading data...\")\n",
    "paths = []\n",
    "for file in os.listdir('data/ETHUSDT'):\n",
    "    if file.endswith('.parquet'):\n",
    "        paths.append(os.path.join('data/ETHUSDT', file))\n",
    "\n",
    "d = concat_parquet_files(paths)\n",
    "print('optimizing strategy ><><><')\n",
    "start = time.time()\n",
    "results = optimize_strategy(d.tail(int(len(d)/40)))\n",
    "print('done optimization')\n",
    "best_params, best_stats = find_best_parameters(results, metric='Sharpe Ratio')\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Stats:\")\n",
    "print(best_stats)\n",
    "\n",
    "\n",
    "\n",
    "print(f'total time is: {time.time()-sy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import vectorbt as vbt\n",
    "from src.utils import crossed_below, crossed_over\n",
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def backtest_strategy(data, tenkan_sen_length, kijun_sen_length, senkou_span_length):\n",
    "    # Step 1: Read the Parquet file\n",
    "    df = data.copy()\n",
    "\n",
    "    # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "\n",
    "    # # Resample to 15-minute intervals (adjust if needed)\n",
    "    # df_resampled = df.resample('1min', on='open_time').agg({\n",
    "    #     'open': 'first',\n",
    "    #     'high': 'max',\n",
    "    #     'low': 'min',\n",
    "    #     'close': 'last'\n",
    "    # })\n",
    "\n",
    "    # # Ensure all columns are properly aggregated\n",
    "    # df = df_resampled.reset_index()\n",
    "\n",
    "    # Ensure the column names are in lower case to match your data labels\n",
    "    df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "\n",
    "    # Step 2: Calculate Ichimoku Cloud components\n",
    "    # Calculate Ichimoku Cloud components with specified parameters\n",
    "    ichimoku_cloud, _ = ta.ichimoku(high=df['high'], low=df['low'], close=df['close'],\n",
    "                                    tenkan=tenkan_sen_length, kijun=kijun_sen_length, senkou=senkou_span_length,\n",
    "                                    include_chikou=False)\n",
    "\n",
    "    # Extract individual components\n",
    "    tenkan_sen = ichimoku_cloud[f'ITS_{tenkan_sen_length}']\n",
    "    kijun_sen = ichimoku_cloud[f'IKS_{kijun_sen_length}']\n",
    "    span_a = ichimoku_cloud[f'ISA_{tenkan_sen_length}']  # Adjust for span A\n",
    "    span_b = ichimoku_cloud[f'ISB_{kijun_sen_length}']\n",
    "\n",
    "    # Step 3: Define Strategy Logic\n",
    "    # Create signals DataFrame\n",
    "    signals = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Define Strategy Logic based on extracted components\n",
    "    # Long when tenkan_sen crosses over kijun_sen, close is above both span A and span B\n",
    "    long_entries = crossed_over(tenkan_sen, kijun_sen) & (df['close'] > span_a) & (df['close'] > span_b)\n",
    "\n",
    "    # Short when tenkan_sen crosses below kijun_sen, close is below both span A and span B\n",
    "    short_entries = crossed_below(tenkan_sen, kijun_sen) & (df['close'] < span_a) & (df['close'] < span_b)\n",
    "\n",
    "    # Exit conditions for long positions: tenkan_sen < kijun_sen\n",
    "    long_exits = crossed_below(tenkan_sen, kijun_sen)\n",
    "\n",
    "    # Exit conditions for short positions: tenkan_sen > kijun_sen\n",
    "    short_exits = crossed_over(tenkan_sen, kijun_sen)\n",
    "\n",
    "    # Assign strategy signals to the signals DataFrame\n",
    "    signals['long'] = long_entries.astype(int)\n",
    "    signals['short'] = short_entries.astype(int)\n",
    "    signals['long_exit'] = long_exits.astype(int)\n",
    "    signals['short_exit'] = short_exits.astype(int)\n",
    "\n",
    "    # Step 4: Backtest the Strategy with vectorbt\n",
    "    # Create portfolio\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        close=df['close'],\n",
    "        high=df['high'],\n",
    "        low=df['low'],\n",
    "        entries=signals['long'],\n",
    "        exits=signals['long_exit'],\n",
    "        short_entries=signals['short'],\n",
    "        short_exits=signals['short_exit'],\n",
    "        freq='1m',  # Frequency for closing positions (adjust as needed)\n",
    "        fees=0.001\n",
    "    )\n",
    "\n",
    "    # Calculate portfolio statistics\n",
    "    stats = portfolio.stats()\n",
    "\n",
    "    return stats\n",
    "\n",
    "def optimize_strategy_worker(params):\n",
    "    data, tenkan_sen_length, kijun_sen_length, senkou_span_length = params\n",
    "    print()\n",
    "    return (tenkan_sen_length, kijun_sen_length, senkou_span_length), backtest_strategy(data, tenkan_sen_length, kijun_sen_length, senkou_span_length)\n",
    "\n",
    "def optimize_strategy(data):\n",
    "    parameter_grid = {\n",
    "        'tenkan_sen_length': range(5, 51, 10),    # Adjust range and step size as needed\n",
    "        'kijun_sen_length': range(20, 61, 10),\n",
    "        'senkou_span_length': range(30, 91, 10)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    params_list = [(data, t, k, s) for t in parameter_grid['tenkan_sen_length']\n",
    "                   for k in parameter_grid['kijun_sen_length']\n",
    "                   for s in parameter_grid['senkou_span_length']]\n",
    "\n",
    "    # Use multiprocessing Pool to parallelize\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        for params, result in pool.imap_unordered(optimize_strategy_worker, params_list):\n",
    "            results[params] = result\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_scatter(results, metric='Sharpe Ratio'):\n",
    "    # Extract parameter values and metric values from results\n",
    "    tenkan_sen_values = [key[0] for key in results.keys()]\n",
    "    kijun_sen_values = [key[1] for key in results.keys()]\n",
    "    senkou_span_values = [key[2] for key in results.keys()]\n",
    "    \n",
    "    # Extract metric values\n",
    "    metric_values = []\n",
    "    for key in results.keys():\n",
    "        stats = results[key]\n",
    "        if metric == 'Sharpe Ratio':\n",
    "            metric_value = stats['Sharpe Ratio']\n",
    "        elif metric == 'Total Return':\n",
    "            metric_value = stats['Total Return']\n",
    "        else:\n",
    "            metric_value = None  # Handle additional metrics if needed\n",
    "        metric_values.append(metric_value)\n",
    "    \n",
    "    # Create scatter plot using Plotly\n",
    "    fig = go.Figure(data=go.Scatter3d(\n",
    "        x=tenkan_sen_values,\n",
    "        y=kijun_sen_values,\n",
    "        z=senkou_span_values,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=12,\n",
    "            color=metric_values,\n",
    "            colorscale='Viridis',  # Choose a colorscale\n",
    "            opacity=0.8,\n",
    "            colorbar=dict(title=metric)\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='Tenkan-sen Length',\n",
    "            yaxis_title='Kijun-sen Length',\n",
    "            zaxis_title='Senkou Span Length'\n",
    "        ),\n",
    "        title=f'Optimization Scatter Plot ({metric})'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "def plot_surface_with_contours(results, metric='Sharpe Ratio'):\n",
    "    # Extract parameter values and metric values from results\n",
    "    parameter_combinations = list(results.keys())\n",
    "    tenkan_sen_values = sorted(set([params[0] for params in parameter_combinations]))\n",
    "    kijun_sen_values = sorted(set([params[1] for params in parameter_combinations]))\n",
    "    senkou_span_values = [params[2] for params in parameter_combinations]\n",
    "    \n",
    "    # Extract metric values\n",
    "    metric_values = []\n",
    "    for params in parameter_combinations:\n",
    "        stats = results[params]\n",
    "        if metric == 'Sharpe Ratio':\n",
    "            metric_value = stats['Sharpe Ratio']\n",
    "        elif metric == 'Total Return [%]':\n",
    "            metric_value = stats.get('Total Return [%]', None)  # Use get() to handle missing keys gracefully\n",
    "        else:\n",
    "            metric_value = None  # Handle additional metrics if needed\n",
    "        metric_values.append(metric_value)\n",
    "    \n",
    "    # Create meshgrid for plotting\n",
    "    X, Y = np.meshgrid(tenkan_sen_values, kijun_sen_values)\n",
    "    \n",
    "    # Ensure Z is initialized with float type for NaNs\n",
    "    Z = np.zeros_like(X, dtype=np.float64)\n",
    "    \n",
    "    # Populate Z with metric values\n",
    "    for i, params in enumerate(parameter_combinations):\n",
    "        tenkan_index = tenkan_sen_values.index(params[0])\n",
    "        kijun_index = kijun_sen_values.index(params[1])\n",
    "        if metric_values[i] is not None:\n",
    "            Z[kijun_index, tenkan_index] = metric_values[i]\n",
    "    \n",
    "    # Create Surface plot using Plotly\n",
    "    fig = go.Figure(data=[go.Surface(\n",
    "        x=X,\n",
    "        y=Y,\n",
    "        z=Z,\n",
    "        contours_z=dict(\n",
    "            show=True,\n",
    "            usecolormap=True,\n",
    "            highlightcolor=\"limegreen\",\n",
    "            project_z=True,\n",
    "            highlightwidth=4,\n",
    "        ),\n",
    "        colorscale='Viridis'  # Choose a colorscale\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='Tenkan-sen Length',\n",
    "            yaxis_title='Kijun-sen Length',\n",
    "            zaxis_title='Metric Value' if metric else 'Senkou Span Length'\n",
    "        ),\n",
    "        title=f'Optimization Surface Plot ({metric})',\n",
    "        autosize=True,\n",
    "        width=800,\n",
    "        height=600,\n",
    "        margin=dict(l=65, r=50, b=65, t=90)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "def concat_parquet_files(file_paths):\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate through each file path\n",
    "    for file_path in file_paths:\n",
    "        # Read the parquet file\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "        df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the list along rows\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return concatenated_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Reading data...\")\n",
    "    paths = [os.path.join('data/ETHUSDT', file) for file in os.listdir('data/ETHUSDT') if file.endswith('.parquet')]\n",
    "    data = concat_parquet_files(paths)\n",
    "\n",
    "    print('Optimizing strategy...')\n",
    "    start = time.time()\n",
    "    results = optimize_strategy(data.tail(int(len(data)/50)))\n",
    "    print('Done optimization')\n",
    "\n",
    "    best_params, best_stats = max(results.items(), key=lambda x: x[1]['Total Return [%]'])\n",
    "\n",
    "    print('\\n')\n",
    "    print(\"---------------------\")\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Stats:\")\n",
    "    print(best_stats)\n",
    "\n",
    "    print(f'Total time taken: {time.time() - start} seconds')\n",
    "    print('\\n')\n",
    "    print(\"---------------------\")\n",
    "    plot_surface_with_contours(results, metric='Total Return')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'vectorbt' has no attribute 'DataLoader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvectorbt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mvbt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mvbt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataLoader\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/ETHUSDT/ETHUSDT-1m-2023-02.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstrategy\u001b[39m(context, data):\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;66;03m# Get open price\u001b[39;00m\n\u001b[1;32m      8\u001b[0m   open_price \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mopen\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'vectorbt' has no attribute 'DataLoader'"
     ]
    }
   ],
   "source": [
    "import vectorbt as vbt\n",
    "\n",
    "# Load data\n",
    "data = vbt.DataLoader.from_parquet(\"data/ETHUSDT/ETHUSDT-1m-2023-02.parquet\")\n",
    "\n",
    "def strategy(context, data):\n",
    "  # Get open price\n",
    "  open_price = data.open\n",
    "\n",
    "  # Open long and short positions with size 1\n",
    "  context.order(data.symbol, size=1)\n",
    "  context.order(data.symbol, size=-1)\n",
    "\n",
    "  # Schedule close orders for both positions after 10 candles\n",
    "  context.schedule_order(dt=10, data=data, symbol=data.symbol, size=1, price=open_price)\n",
    "  context.schedule_order(dt=10, data=data, symbol=data.symbol, size=-1, price=open_price)\n",
    "\n",
    "# Create portfolio\n",
    "portfolio = vbt.Portfolio.from_orders(strategy)\n",
    "\n",
    "# Run simulation\n",
    "results = vbt.run(portfolio, data)\n",
    "\n",
    "# Analyze results (optional)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
