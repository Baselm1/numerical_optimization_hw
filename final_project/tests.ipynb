{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL for the 24hr ticker price change statistics endpoint\n",
    "url = \"https://api.binance.com/api/v3/ticker/24hr\"\n",
    "\n",
    "# Make the request to get the ticker information\n",
    "response = requests.get(url)\n",
    "ticker_data = response.json()\n",
    "\n",
    "# Print trading volume for each market\n",
    "print(\"24hr Trading Volumes:\")\n",
    "for ticker in ticker_data:\n",
    "    symbol = ticker['symbol']\n",
    "    volume = ticker['volume']\n",
    "    quote_volume = ticker['quoteVolume']\n",
    "    print(f\"Market: {symbol}, Volume: {volume}, Quote Volume: {quote_volume}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL for the Binance exchange information endpoint\n",
    "url = \"https://api.binance.com/api/v3/exchangeInfo\"\n",
    "\n",
    "# Make the request to get the exchange info\n",
    "response = requests.get(url)\n",
    "exchange_info = response.json()\n",
    "\n",
    "# Extract unique quote assets\n",
    "quote_assets = set()\n",
    "for symbol in exchange_info['symbols']:\n",
    "    quote_assets.add(symbol['quoteAsset'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URLs\n",
    "exchange_info_url = \"https://api.binance.com/api/v3/exchangeInfo\"\n",
    "ticker_24hr_url = \"https://api.binance.com/api/v3/ticker/24hr\"\n",
    "\n",
    "# Send requests to both URLs\n",
    "exchange_info_response = requests.get(exchange_info_url)\n",
    "ticker_24hr_response = requests.get(ticker_24hr_url)\n",
    "\n",
    "# Parse the JSON responses\n",
    "exchange_info = exchange_info_response.json()\n",
    "ticker_24hr_info = ticker_24hr_response.json()\n",
    "\n",
    "# Create a dictionary to map symbols to their base and quote assets\n",
    "symbol_info_map = {}\n",
    "for symbol_info in exchange_info['symbols']:\n",
    "    symbol = symbol_info['symbol']\n",
    "    base_asset = symbol_info['baseAsset']\n",
    "    quote_asset = symbol_info['quoteAsset']\n",
    "    symbol_info_map[symbol] = {\n",
    "        'symbol': symbol,\n",
    "        'baseAsset': base_asset,\n",
    "        'quoteAsset': quote_asset\n",
    "    }\n",
    "\n",
    "# Create the final list of dictionaries with the required keys\n",
    "result = []\n",
    "for ticker in ticker_24hr_info:\n",
    "    symbol = ticker['symbol']\n",
    "    if symbol in symbol_info_map:\n",
    "        combined_info = symbol_info_map[symbol]\n",
    "        combined_info['priceChangePercent'] = ticker['priceChangePercent']\n",
    "        combined_info['quoteVolume'] = ticker['quoteVolume']\n",
    "        result.append(combined_info)\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.data_handler as m \n",
    "\n",
    "handler = m.DataHandler(min_volume=10e6)\n",
    "\n",
    "result = handler.get_trading_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in result:\n",
    "    print(\"------------------------------\")\n",
    "    print(pair)\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in result:\n",
    "    print(\"------------------------------\")\n",
    "    print(pair)\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "async def fetch(session, url):\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            if response.status == 200:\n",
    "                return await response.json()\n",
    "            else:\n",
    "                return {'error': f\"Failed to fetch {url}: {response.status}\"}\n",
    "    except aiohttp.ClientError as e:\n",
    "        return {'error': f\"Request failed for {url}: {str(e)}\"}\n",
    "\n",
    "async def fetch_all(urls):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch(session, url) for url in urls]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "def process_responses(responses):\n",
    "    responses_list = []\n",
    "    for response in responses:\n",
    "        if 'error' in response:\n",
    "            print(response['error'])\n",
    "        else:\n",
    "            responses_list.append(response)\n",
    "    return responses_list\n",
    "\n",
    "urls = [\n",
    "    'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/1m/BTCUSDT-1m-2024-05.zip',\n",
    "    'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/1m/BTCUSDT-1m-2024-04.zip',\n",
    "    'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/1m/BTCUSDT-1m-2024-03.zip',\n",
    "    'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/1m/BTCUSDT-1m-2024-02.zip',\n",
    "    'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/1m/BTCUSDT-1m-2024-01.zip'\n",
    "]\n",
    "\n",
    "async def main():\n",
    "    responses = await fetch_all(urls)\n",
    "    return process_responses(responses)\n",
    "\n",
    "# Run the asyncio event loop\n",
    "l = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "x = os.listdir('data/downloads')\n",
    "\n",
    "unique = []\n",
    "\n",
    "for file in x: \n",
    "    p = file.split('-')[0]\n",
    "    unique.append(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "import vectorbt as vbt\n",
    "\n",
    "# Step 1: Read the Parquet file\n",
    "df = pd.read_parquet('data/ETHUSDT/ETHUSDT-1m-2018-01.parquet')\n",
    "\n",
    "# Ensure the column names are in lower case to match your data labels\n",
    "df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "\n",
    "# Set open_time as the DataFrame index\n",
    "df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "df.set_index('open_time', inplace=True)\n",
    "\n",
    "# Step 2: Calculate Ichimoku Cloud components\n",
    "# Calculate Ichimoku Cloud components\n",
    "ichimoku_cloud, _ = df.ta.ichimoku(tenkan_sen=True, kijun_sen=True, senkou_span=True, chikou_span=False)\n",
    "\n",
    "# Extract individual components\n",
    "tenkan_sen = ichimoku_cloud['ITS_9']\n",
    "kijun_sen = ichimoku_cloud['IKS_26']\n",
    "span_a = ichimoku_cloud['ISA_9']\n",
    "span_b = ichimoku_cloud['ISB_26']\n",
    "\n",
    "# Step 3: Define Strategy Logic\n",
    "# Strategy rules: Long when close is above both span A and span B, and tenkan_sen is above kijun_sen\n",
    "long_entries = (df['close'] > span_a) & (df['close'] > span_b) & (tenkan_sen > kijun_sen)\n",
    "# Short when close is below both span A and span B, and tenkan_sen is below kijun_sen\n",
    "short_entries = (df['close'] < span_a) & (df['close'] < span_b) & (tenkan_sen < kijun_sen)\n",
    "\n",
    "# Step 4: Backtest the Strategy with vectorbt\n",
    "# Create signals DataFrame\n",
    "signals = pd.DataFrame(index=df.index)\n",
    "signals['long'] = long_entries.astype(int)\n",
    "signals['short'] = short_entries.astype(int)\n",
    "\n",
    "# Create portfolio\n",
    "portfolio = vbt.Portfolio.from_signals(\n",
    "    close=df['close'],\n",
    "    entries=signals['long'],\n",
    "    exits=signals['short'],\n",
    "    freq='1m'  # Daily frequency for closing positions\n",
    ")\n",
    "\n",
    "# Calculate portfolio statistics\n",
    "stats = portfolio.stats()\n",
    "\n",
    "# Step 5: Plotting the Backtest Results\n",
    "# Plot the portfolio performance\n",
    "portfolio.plot().show()\n",
    "\n",
    "# Plot entries and exits on price chart\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.plot(df.index, df['close'], label='Close')\n",
    "ax.plot(tenkan_sen.index, tenkan_sen, label='Tenkan-sen', linestyle='--')\n",
    "ax.plot(kijun_sen.index, kijun_sen, label='Kijun-sen', linestyle='--')\n",
    "ax.plot(span_a.index, span_a, label='Span A', linestyle='--')\n",
    "ax.plot(span_b.index, span_b, label='Span B', linestyle='--')\n",
    "\n",
    "# Plot entry and exit signals\n",
    "ax.plot(signals[signals['long'] == 1].index, df.loc[signals['long'] == 1, 'close'], '^', markersize=10, color='g', label='Long Entry')\n",
    "ax.plot(signals[signals['short'] == 1].index, df.loc[signals['short'] == 1, 'close'], 'v', markersize=10, color='r', label='Short Entry')\n",
    "\n",
    "ax.legend()\n",
    "plt.title('Ichimoku Cloud Strategy')\n",
    "plt.show()\n",
    "\n",
    "# Print portfolio statistics\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/ETHUSDT/ETHUSDT-1m-2018-01.parquet')\n",
    "\n",
    "# Ensure the column names are in lower case to match your data labels\n",
    "df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "\n",
    "# Set open_time as the DataFrame index\n",
    "df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "df.set_index('open_time', inplace=True)\n",
    "\n",
    "# Step 2: Calculate Ichimoku Cloud components\n",
    "# Calculate Ichimoku Cloud components\n",
    "ichimoku_cloud, _ = df.ta.ichimoku(tenkan_sen=True, kijun_sen=True, senkou_span=True, chikou_span=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ichimoku_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "import vectorbt as vbt\n",
    "\n",
    "# Step 1: Read the Parquet file\n",
    "df = pd.read_parquet('data/ETHUSDT/ETHUSDT-1m-2018-01.parquet')\n",
    "\n",
    "df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "\n",
    "# Resample to 5-minute intervals\n",
    "\n",
    "df_resampled = df.resample('1min', on='open_time').agg({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last'\n",
    "})\n",
    "\n",
    "\n",
    "# Ensure all columns are properly aggregated\n",
    "df = df_resampled.reset_index(inplace=False)\n",
    "\n",
    "\n",
    "# Ensure the column names are in lower case to match your data labels\n",
    "df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "\n",
    "# Step 2: Calculate Ichimoku Cloud components\n",
    "# Calculate Ichimoku Cloud components\n",
    "ichimoku_cloud, _ = df.ta.ichimoku(tenkan_sen=True, kijun_sen=True, senkou_span=True, chikou_span=False)\n",
    "\n",
    "# Extract individual components\n",
    "tenkan_sen = ichimoku_cloud['ITS_9']\n",
    "kijun_sen = ichimoku_cloud['IKS_26']\n",
    "span_a = ichimoku_cloud['ISA_9']\n",
    "span_b = ichimoku_cloud['ISB_26']\n",
    "\n",
    "# Step 3: Define Strategy Logic\n",
    "# Strategy rules: Long when close is above both span A and span B, and tenkan_sen is above kijun_sen\n",
    "long_entries = (df['close'] > span_a) & (df['close'] > span_b) & (tenkan_sen > kijun_sen)\n",
    "# Short when close is below both span A and span B, and tenkan_sen is below kijun_sen\n",
    "short_entries = (df['close'] < span_a) & (df['close'] < span_b) & (tenkan_sen < kijun_sen)\n",
    "\n",
    "# Exit conditions for long positions\n",
    "long_exits = ~(tenkan_sen > kijun_sen) \n",
    "\n",
    "# Exit conditions for short positions\n",
    "short_exits = ~(tenkan_sen < kijun_sen) \n",
    "\n",
    "# Step 4: Backtest the Strategy with vectorbt\n",
    "# Create signals DataFrame\n",
    "signals = pd.DataFrame(index=df.index)\n",
    "signals['long'] = long_entries.astype(int)\n",
    "signals['short'] = short_entries.astype(int)\n",
    "\n",
    "# Create exit signals\n",
    "signals['long_exit'] = long_exits.astype(int)\n",
    "signals['short_exit'] = short_exits.astype(int)\n",
    "\n",
    "# Create portfolio\n",
    "portfolio = vbt.Portfolio.from_signals(\n",
    "    close=df['close'],\n",
    "    entries=signals['long'],\n",
    "    exits=signals['long_exit'],\n",
    "    short_entries=signals['short'],\n",
    "    short_exits=signals['short_exit'],\n",
    "    freq='1m'  # Daily frequency for closing positions\n",
    ")\n",
    "\n",
    "# Calculate portfolio statistics\n",
    "stats = portfolio.stats()\n",
    "\n",
    "# Step 5: Plotting the Backtest Results\n",
    "# Plot the portfolio performance\n",
    "portfolio.plot().show()\n",
    "\n",
    "# Plot entries and exits on price chart\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.plot(df.index, df['close'], label='Close')\n",
    "ax.plot(tenkan_sen.index, tenkan_sen, label='Tenkan-sen', linestyle='--')\n",
    "ax.plot(kijun_sen.index, kijun_sen, label='Kijun-sen', linestyle='--')\n",
    "ax.plot(span_a.index, span_a, label='Span A', linestyle='--')\n",
    "ax.plot(span_b.index, span_b, label='Span B', linestyle='--')\n",
    "\n",
    "# Plot entry signals\n",
    "ax.plot(signals[signals['long'] == 1].index, df.loc[signals['long'] == 1, 'close'], '^', markersize=10, color='g', label='Long Entry')\n",
    "ax.plot(signals[signals['short'] == 1].index, df.loc[signals['short'] == 1, 'close'], 'v', markersize=10, color='r', label='Short Entry')\n",
    "\n",
    "# Plot exit signals\n",
    "ax.plot(signals[signals['long_exit'] == 1].index, df.loc[signals['long_exit'] == 1, 'close'], 'o', markersize=7, color='g', label='Long Exit')\n",
    "ax.plot(signals[signals['short_exit'] == 1].index, df.loc[signals['short_exit'] == 1, 'close'], 'o', markersize=7, color='r', label='Short Exit')\n",
    "\n",
    "ax.legend()\n",
    "plt.title('Ichimoku Cloud Strategy with Exit Signals')\n",
    "plt.show()\n",
    "\n",
    "# Print portfolio statistics\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "import vectorbt as vbt\n",
    "\n",
    "# Step 1: Read the Parquet file\n",
    "df = pd.read_parquet('data/ETHUSDT/ETHUSDT-1m-2023-09.parquet')\n",
    "\n",
    "# Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "\n",
    "# Resample to 5-minute intervals\n",
    "df_resampled = df.resample('1min', on='open_time').agg({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last'\n",
    "})\n",
    "\n",
    "# Ensure all columns are properly aggregated\n",
    "df = df_resampled.reset_index()\n",
    "\n",
    "# Ensure the column names are in lower case to match your data labels\n",
    "df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "\n",
    "# Step 2: Calculate Ichimoku Cloud components\n",
    "# Calculate Ichimoku Cloud components\n",
    "ichimoku_cloud, _ = df.ta.ichimoku(tenkan_sen=True, kijun_sen=True, senkou_span=True, chikou_span=False)\n",
    "\n",
    "# Extract individual components\n",
    "tenkan_sen = ichimoku_cloud['ITS_9']\n",
    "kijun_sen = ichimoku_cloud['IKS_26']\n",
    "span_a = ichimoku_cloud['ISA_9']\n",
    "span_b = ichimoku_cloud['ISB_26']\n",
    "\n",
    "# Step 3: Define Strategy Logic\n",
    "\n",
    "# Strategy rules: Long when close is above both span A and span B, tenkan_sen is above kijun_sen, and RSI < 30\n",
    "long_entries = (df['close'] > span_a) & (df['close'] > span_b) & (tenkan_sen > kijun_sen) \n",
    "# Short when close is below both span A and span B, tenkan_sen is below kijun_sen, and RSI > 70\n",
    "short_entries = (df['close'] < span_a) & (df['close'] < span_b) & (tenkan_sen < kijun_sen)\n",
    "\n",
    "# Exit conditions for long positions: tenkan_sen < kijun_sen or RSI > 70\n",
    "long_exits = (tenkan_sen < kijun_sen) \n",
    "# Exit conditions for short positions: tenkan_sen > kijun_sen or RSI < 30\n",
    "short_exits = (tenkan_sen > kijun_sen) \n",
    "\n",
    "# Step 4: Backtest the Strategy with vectorbt\n",
    "# Create signals DataFrame\n",
    "signals = pd.DataFrame(index=df.index)\n",
    "signals['long'] = long_entries.astype(int)\n",
    "signals['short'] = short_entries.astype(int)\n",
    "\n",
    "# Create exit signals\n",
    "signals['long_exit'] = long_exits.astype(int)\n",
    "signals['short_exit'] = short_exits.astype(int)\n",
    "\n",
    "# Create portfolio\n",
    "portfolio = vbt.Portfolio.from_signals(\n",
    "    close=df['close'],\n",
    "    entries=signals['long'],\n",
    "    exits=signals['long_exit'],\n",
    "    short_entries=signals['short'],\n",
    "    short_exits=signals['short_exit'],\n",
    "    freq='1m'  # 5-minute frequency for closing positions\n",
    ")\n",
    "\n",
    "# Calculate portfolio statistics\n",
    "stats = portfolio.stats()\n",
    "\n",
    "# Step 5: Plotting the Backtest Results\n",
    "# Plot the portfolio performance\n",
    "portfolio.plot().show()\n",
    "\n",
    "# Plot entries and exits on price chart\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.plot(df['open_time'], df['close'], label='Close')\n",
    "ax.plot(tenkan_sen.index, tenkan_sen, label='Tenkan-sen', linestyle='--')\n",
    "ax.plot(kijun_sen.index, kijun_sen, label='Kijun-sen', linestyle='--')\n",
    "ax.plot(span_a.index, span_a, label='Span A', linestyle='--')\n",
    "ax.plot(span_b.index, span_b, label='Span B', linestyle='--')\n",
    "\n",
    "# Plot entry signals\n",
    "ax.plot(signals[signals['long'] == 1].index, df.loc[signals['long'] == 1, 'close'], '^', markersize=10, color='g', label='Long Entry')\n",
    "ax.plot(signals[signals['short'] == 1].index, df.loc[signals['short'] == 1, 'close'], 'v', markersize=10, color='r', label='Short Entry')\n",
    "\n",
    "# Plot exit signals\n",
    "ax.plot(signals[signals['long_exit'] == 1].index, df.loc[signals['long_exit'] == 1, 'close'], 'o', markersize=7, color='g', label='Long Exit')\n",
    "ax.plot(signals[signals['short_exit'] == 1].index, df.loc[signals['short_exit'] == 1, 'close'], 'o', markersize=7, color='r', label='Short Exit')\n",
    "\n",
    "ax.legend()\n",
    "plt.title('Ichimoku Cloud Strategy with RSI Conditions')\n",
    "plt.show()\n",
    "\n",
    "# Print portfolio statistics\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "import vectorbt as vbt\n",
    "import os \n",
    "\n",
    "\n",
    "def backtest(data):\n",
    "    # Step 1: Read the Parquet file\n",
    "    df = data\n",
    "\n",
    "    # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    \n",
    "    # Resample to 5-minute intervals\n",
    "    df_resampled = df.resample('1min', on='open_time').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    })\n",
    "    \n",
    "    # Ensure all columns are properly aggregated\n",
    "    df = df_resampled.reset_index()\n",
    "    \n",
    "    # Ensure the column names are in lower case to match your data labels\n",
    "    df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "    \n",
    "    # Step 2: Calculate Ichimoku Cloud components\n",
    "    # Calculate Ichimoku Cloud components\n",
    "    ichimoku_cloud, _ = df.ta.ichimoku(tenkan_sen=True, kijun_sen=True, senkou_span=True, chikou_span=False)\n",
    "    \n",
    "    # Extract individual components\n",
    "    tenkan_sen = ichimoku_cloud['ITS_9']\n",
    "    kijun_sen = ichimoku_cloud['IKS_26']\n",
    "    span_a = ichimoku_cloud['ISA_9']\n",
    "    span_b = ichimoku_cloud['ISB_26']\n",
    "    \n",
    "    # Step 3: Define Strategy Logic\n",
    "    # Calculate RSI\n",
    "    df['rsi'] = ta.rsi(df['close'], length=14)\n",
    "    \n",
    "    # Define Strategy Logic\n",
    "    # Long when close is above both span A and span B, tenkan_sen is above kijun_sen, and RSI < 30\n",
    "    long_entries = (df['close'] > span_a) & (df['close'] > span_b) & (tenkan_sen > kijun_sen)\n",
    "    # Short when close is below both span A and span B, tenkan_sen is below kijun_sen, and RSI > 70\n",
    "    short_entries = (df['close'] < span_a) & (df['close'] < span_b) & (tenkan_sen < kijun_sen)\n",
    "    \n",
    "    # Exit conditions for long positions: tenkan_sen < kijun_sen or RSI > 70\n",
    "    long_exits = (tenkan_sen < kijun_sen) | (df['rsi'] > 70)\n",
    "    # Exit conditions for short positions: tenkan_sen > kijun_sen or RSI < 30\n",
    "    short_exits = (tenkan_sen > kijun_sen) | (df['rsi'] < 30) \n",
    "    \n",
    "    # Step 4: Backtest the Strategy with vectorbt\n",
    "    # Create signals DataFrame\n",
    "    signals = pd.DataFrame(index=df.index)\n",
    "    signals['long'] = long_entries.astype(int)\n",
    "    signals['short'] = short_entries.astype(int)\n",
    "    \n",
    "    # Create exit signals\n",
    "    signals['long_exit'] = long_exits.astype(int)\n",
    "    signals['short_exit'] = short_exits.astype(int)\n",
    "    \n",
    "    # Create portfolio\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        close=df['close'],\n",
    "        entries=signals['long'],\n",
    "        exits=signals['long_exit'],\n",
    "        short_entries=signals['short'],\n",
    "        short_exits=signals['short_exit'],\n",
    "        freq='1m',  # 5-minute frequency for closing positions\n",
    "    )\n",
    "    \n",
    "    # Calculate portfolio statistics\n",
    "    stats = portfolio.stats()\n",
    "    \n",
    "    # Step 5: Plotting the Backtest Results\n",
    "    # Plot the portfolio performance\n",
    "    portfolio.plot().show()\n",
    "    # Print portfolio statistics\n",
    "    print(stats)\n",
    "\n",
    "def concat_parquet_files(file_paths):\n",
    "    dfs = []\n",
    "    \n",
    "    # Iterate through each file path\n",
    "    for file_path in file_paths:\n",
    "        # Read the parquet file\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "        df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames in the list along rows\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return concatenated_df\n",
    "\n",
    "count=0\n",
    "paths = []\n",
    "for file in os.listdir('data/DOGEUSDT'):\n",
    "    if file.endswith('.parquet'):\n",
    "        paths.append(os.path.join('data/DOGEUSDT', file))\n",
    "\n",
    "\n",
    "eth_data = concat_parquet_files(paths)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "import vectorbt as vbt\n",
    "import os \n",
    "import src.utils as ut\n",
    "\n",
    "def backtest(file):\n",
    "    # Step 1: Read the Parquet file\n",
    "    df = pd.read_parquet(file)\n",
    "\n",
    "    # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    \n",
    "    # Resample to 1-minute intervals (adjust if needed)\n",
    "    df_resampled = df.resample('15min', on='open_time').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    })\n",
    "    \n",
    "    # Ensure all columns are properly aggregated\n",
    "    df = df_resampled.reset_index()\n",
    "    \n",
    "    # Ensure the column names are in lower case to match your data labels\n",
    "    df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "    \n",
    "    # Step 2: Calculate Ichimoku Cloud components\n",
    "    # Calculate Ichimoku Cloud components\n",
    "    ichimoku_cloud, _ = df.ta.ichimoku(tenkan_sen=True, kijun_sen=True, senkou_span=True, chikou_span=False)\n",
    "    \n",
    "    # Extract individual components\n",
    "    tenkan_sen = ichimoku_cloud['ITS_9']\n",
    "    kijun_sen = ichimoku_cloud['IKS_26']\n",
    "    span_a = ichimoku_cloud['ISA_9']\n",
    "    span_b = ichimoku_cloud['ISB_26']\n",
    "    \n",
    "    # Step 3: Calculate RSI\n",
    "    df['rsi'] = ta.rsi(df['close'], length=14)\n",
    "    \n",
    "    # Step 4: Define Strategy Logic\n",
    "    # Create signals DataFrame\n",
    "    signals = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Define Strategy Logic\n",
    "    # Long when tenkan_sen crosses over kijun_sen, close is above both span A and span B, and RSI < 30\n",
    "    long_entries = (ut.crossed_over(tenkan_sen, kijun_sen)) & (df['close'] > span_a) & (df['close'] > span_b) \n",
    "    \n",
    "    # Short when tenkan_sen crosses below kijun_sen, close is below both span A and span B, and RSI > 70\n",
    "    short_entries = (ut.crossed_below(tenkan_sen, kijun_sen)) & (df['close'] < span_a) & (df['close'] < span_b) \n",
    "    \n",
    "    # Exit conditions for long positions: tenkan_sen < kijun_sen or RSI > 70\n",
    "    long_exits = ut.crossed_below(tenkan_sen, kijun_sen)\n",
    "    # Exit conditions for short positions: tenkan_sen > kijun_sen or RSI < 30\n",
    "    short_exits = ut.crossed_over(tenkan_sen, kijun_sen) \n",
    "    \n",
    "    # Assign strategy signals to the signals DataFrame\n",
    "    signals['long'] = long_entries.astype(int)\n",
    "    signals['short'] = short_entries.astype(int)\n",
    "    signals['long_exit'] = long_exits.astype(int)\n",
    "    signals['short_exit'] = short_exits.astype(int)\n",
    "    \n",
    "    # Step 5: Backtest the Strategy with vectorbt\n",
    "    # Create portfolio\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        close=df['close'],\n",
    "        entries=signals['long'],\n",
    "        exits=signals['long_exit'],\n",
    "        short_entries=signals['short'],\n",
    "        short_exits=signals['short_exit'],\n",
    "        freq='1min',  # Frequency for closing positions (adjust as needed)\n",
    "        fees=0.001\n",
    "    )\n",
    "    \n",
    "    # Calculate portfolio statistics\n",
    "    stats = portfolio.stats()\n",
    "    \n",
    "    # Step 6: Plotting the Backtest Results\n",
    "    # Plot the portfolio performance\n",
    "    portfolio.plot().show()\n",
    "    # Print portfolio statistics\n",
    "    print(stats)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def backtest_data(data):\n",
    "    # Step 1: Read the Parquet file\n",
    "    df = data\n",
    "\n",
    "    # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    \n",
    "    # Resample to 1-minute intervals (adjust if needed)\n",
    "    df_resampled = df.resample('15min', on='open_time').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    })\n",
    "    \n",
    "    # Ensure all columns are properly aggregated\n",
    "    df = df_resampled.reset_index()\n",
    "    \n",
    "    # Ensure the column names are in lower case to match your data labels\n",
    "    df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "    \n",
    "    # Step 2: Calculate Ichimoku Cloud components\n",
    "    # Calculate Ichimoku Cloud components\n",
    "    ichimoku_cloud, _ = df.ta.ichimoku(tenkan_sen=True, kijun_sen=True, senkou_span=True, chikou_span=False)\n",
    "    \n",
    "    # Extract individual components\n",
    "    tenkan_sen = ichimoku_cloud['ITS_9']\n",
    "    kijun_sen = ichimoku_cloud['IKS_26']\n",
    "    span_a = ichimoku_cloud['ISA_9']\n",
    "    span_b = ichimoku_cloud['ISB_26']\n",
    "    \n",
    "    # Step 3: Calculate RSI\n",
    "    df['rsi'] = ta.rsi(df['close'], length=14)\n",
    "    \n",
    "    # Step 4: Define Strategy Logic\n",
    "    # Create signals DataFrame\n",
    "    signals = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Define Strategy Logic\n",
    "    # Long when tenkan_sen crosses over kijun_sen, close is above both span A and span B, and RSI < 30\n",
    "    long_entries = (ut.crossed_over(tenkan_sen, kijun_sen)) & (df['close'] > span_a) & (df['close'] > span_b) \n",
    "    \n",
    "    # Short when tenkan_sen crosses below kijun_sen, close is below both span A and span B, and RSI > 70\n",
    "    short_entries = (ut.crossed_below(tenkan_sen, kijun_sen)) & (df['close'] < span_a) & (df['close'] < span_b) \n",
    "    \n",
    "    # Exit conditions for long positions: tenkan_sen < kijun_sen or RSI > 70\n",
    "    long_exits = ut.crossed_below(tenkan_sen, kijun_sen)\n",
    "    # Exit conditions for short positions: tenkan_sen > kijun_sen or RSI < 30\n",
    "    short_exits = ut.crossed_over(tenkan_sen, kijun_sen) \n",
    "    \n",
    "    # Assign strategy signals to the signals DataFrame\n",
    "    signals['long'] = long_entries.astype(int)\n",
    "    signals['short'] = short_entries.astype(int)\n",
    "    signals['long_exit'] = long_exits.astype(int)\n",
    "    signals['short_exit'] = short_exits.astype(int)\n",
    "    \n",
    "    # Step 5: Backtest the Strategy with vectorbt\n",
    "    # Create portfolio\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        close=df['close'],\n",
    "        entries=signals['long'],\n",
    "        exits=signals['long_exit'],\n",
    "        short_entries=signals['short'],\n",
    "        short_exits=signals['short_exit'],\n",
    "        freq='1min',  # Frequency for closing positions (adjust as needed)\n",
    "        fees=0.001\n",
    "    )\n",
    "    \n",
    "    # Calculate portfolio statistics\n",
    "    stats = portfolio.stats()\n",
    "    \n",
    "    # Step 6: Plotting the Backtest Results\n",
    "    # Plot the portfolio performance\n",
    "    portfolio.plot().show()\n",
    "    # Print portfolio statistics\n",
    "    print(stats)\n",
    "\n",
    "def concat_parquet_files(file_paths):\n",
    "    dfs = []\n",
    "    \n",
    "    # Iterate through each file path\n",
    "    for file_path in file_paths:\n",
    "        # Read the parquet file\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "        df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames in the list along rows\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return concatenated_df\n",
    "\n",
    "paths = []\n",
    "for file in os.listdir('data/DOGEUSDT'):\n",
    "    if file.endswith('.parquet'):\n",
    "        paths.append(os.path.join('data/DOGEUSDT', file))\n",
    "\n",
    "d = concat_parquet_files(paths)\n",
    "\n",
    "backtest_data(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "####### OPTIMIZATION #########\n",
    "##############################\n",
    "\n",
    "import pandas as pd \n",
    "import pandas_ta as ta \n",
    "import vectorbt as vbt\n",
    "import numpy as np\n",
    "from src.utils import crossed_below, crossed_over\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def backtest_strategy(data, tenkan_sen_length, kijun_sen_length, senkou_span_length):\n",
    "    # Step 1: Read the Parquet file\n",
    "    df = data.copy()\n",
    "\n",
    "    # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    \n",
    "    # Resample to 15-minute intervals (adjust if needed)\n",
    "    df_resampled = df.resample('15min', on='open_time').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    })\n",
    "    \n",
    "    # Ensure all columns are properly aggregated\n",
    "    df = df_resampled.reset_index()\n",
    "    \n",
    "    # Ensure the column names are in lower case to match your data labels\n",
    "    df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "    \n",
    "    # Step 2: Calculate Ichimoku Cloud components\n",
    "    # Calculate Ichimoku Cloud components with specified parameters\n",
    "    ichimoku_cloud, _ = ta.ichimoku(high= df['high'], low=df['low'], close=df['close'], tenkan=tenkan_sen_length, kijun=kijun_sen_length, senkou=senkou_span_length, include_chikou=False)\n",
    "\n",
    "    # ichimoku_cloud.rename(columns={'ISA_9': f'ISA_{senkou_span_length//2}',\n",
    "    #                                 'ISB_26':f'ISB_{senkou_span_length}',\n",
    "    #                                 'ITS_9':f'ITS_{tenkan_sen_length}',\n",
    "    #                                 'IKS_26':f'IKS_{kijun_sen_length}' }, inplace=True)\n",
    "    # ichimoku_cloud.drop(columns=['ICS_26'])\n",
    "    \n",
    "    # Extract individual components\n",
    "    tenkan_sen = ichimoku_cloud[f'ITS_{tenkan_sen_length}']\n",
    "    kijun_sen = ichimoku_cloud[f'IKS_{kijun_sen_length}']\n",
    "    span_a = ichimoku_cloud[f'ISA_{tenkan_sen_length}']  # Adjust for span A\n",
    "    span_b = ichimoku_cloud[f'ISB_{kijun_sen_length}']\n",
    "    \n",
    "    # Step 3: Define Strategy Logic\n",
    "    # Create signals DataFrame\n",
    "    signals = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Define Strategy Logic based on extracted components\n",
    "    # Long when tenkan_sen crosses over kijun_sen, close is above both span A and span B\n",
    "    long_entries = crossed_over(tenkan_sen, kijun_sen) & (df['close'] > span_a) & (df['close'] > span_b)\n",
    "    \n",
    "    # Short when tenkan_sen crosses below kijun_sen, close is below both span A and span B\n",
    "    short_entries = crossed_below(tenkan_sen, kijun_sen) & (df['close'] < span_a) & (df['close'] < span_b) \n",
    "    \n",
    "    # Exit conditions for long positions: tenkan_sen < kijun_sen\n",
    "    long_exits = crossed_below(tenkan_sen, kijun_sen)\n",
    "    \n",
    "    # Exit conditions for short positions: tenkan_sen > kijun_sen\n",
    "    short_exits = crossed_over(tenkan_sen, kijun_sen)\n",
    "    \n",
    "    # Assign strategy signals to the signals DataFrame\n",
    "    signals['long'] = long_entries.astype(int)\n",
    "    signals['short'] = short_entries.astype(int)\n",
    "    signals['long_exit'] = long_exits.astype(int)\n",
    "    signals['short_exit'] = short_exits.astype(int)\n",
    "    \n",
    "    # Step 4: Backtest the Strategy with vectorbt\n",
    "    # Create portfolio\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        close=df['close'],\n",
    "        entries=signals['long'],\n",
    "        exits=signals['long_exit'],\n",
    "        short_entries=signals['short'],\n",
    "        short_exits=signals['short_exit'],\n",
    "        freq='15min',  # Frequency for closing positions (adjust as needed)\n",
    "        fees=0.001\n",
    "    )\n",
    "    \n",
    "    # Calculate portfolio statistics\n",
    "    stats = portfolio.stats()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def optimize_strategy(data):\n",
    "    parameter_grid = {\n",
    "        'tenkan_sen_length': range(5, 51, 5),    # Adjust range and step size as needed\n",
    "        'kijun_sen_length': range(20, 61, 5),\n",
    "        'senkou_span_length': range(30, 91, 5)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for tenkan_sen_length in parameter_grid['tenkan_sen_length']:\n",
    "        for kijun_sen_length in parameter_grid['kijun_sen_length']:\n",
    "            for senkou_span_length in parameter_grid['senkou_span_length']:\n",
    "                # Run backtest with current parameter combination\n",
    "                print(f\"Testing Params: {tenkan_sen_length} ,{kijun_sen_length} ,{senkou_span_length}\")\n",
    "                stats = backtest_strategy(data, tenkan_sen_length, kijun_sen_length, senkou_span_length)\n",
    "                \n",
    "                # Store results\n",
    "                results[(tenkan_sen_length, kijun_sen_length, senkou_span_length)] = stats\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def find_best_parameters(results, metric='Sharpe Ratio'):\n",
    "    # Convert results dictionary to DataFrame for easier manipulation\n",
    "    results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "    \n",
    "    # Find parameters that maximize the chosen metric\n",
    "    if metric == 'Sharpe Ratio':\n",
    "        best_params = results_df['Sharpe Ratio'].idxmax()\n",
    "    elif metric == 'Total Return':\n",
    "        best_params = results_df['Total Return'].idxmax()\n",
    "    # Add more metrics as needed\n",
    "    \n",
    "    best_stats = results_df.loc[best_params]\n",
    "    \n",
    "    return best_params, best_stats\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_scatter(results, metric='Sharpe Ratio'):\n",
    "    # Extract parameter values and metric values from results\n",
    "    tenkan_sen_values = [key[0] for key in results.keys()]\n",
    "    kijun_sen_values = [key[1] for key in results.keys()]\n",
    "    senkou_span_values = [key[2] for key in results.keys()]\n",
    "    \n",
    "    # Extract metric values\n",
    "    metric_values = []\n",
    "    for key in results.keys():\n",
    "        stats = results[key]\n",
    "        if metric == 'Sharpe Ratio':\n",
    "            metric_value = stats['Sharpe Ratio']\n",
    "        elif metric == 'Total Return':\n",
    "            metric_value = stats['Total Return']\n",
    "        else:\n",
    "            metric_value = None  # Handle additional metrics if needed\n",
    "        metric_values.append(metric_value)\n",
    "    \n",
    "    # Create scatter plot using Plotly\n",
    "    fig = go.Figure(data=go.Scatter3d(\n",
    "        x=tenkan_sen_values,\n",
    "        y=kijun_sen_values,\n",
    "        z=senkou_span_values,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=12,\n",
    "            color=metric_values,\n",
    "            colorscale='Viridis',  # Choose a colorscale\n",
    "            opacity=0.8,\n",
    "            colorbar=dict(title=metric)\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='Tenkan-sen Length',\n",
    "            yaxis_title='Kijun-sen Length',\n",
    "            zaxis_title='Senkou Span Length'\n",
    "        ),\n",
    "        title=f'Optimization Scatter Plot ({metric})'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "def concat_parquet_files(file_paths):\n",
    "    dfs = []\n",
    "    \n",
    "    # Iterate through each file path\n",
    "    for file_path in file_paths:\n",
    "        # Read the parquet file\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "        df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames in the list along rows\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return concatenated_df\n",
    "\n",
    "print(\"reading data...\")\n",
    "paths = []\n",
    "for file in os.listdir('data/ETHUSDT'):\n",
    "    if file.endswith('.parquet'):\n",
    "        paths.append(os.path.join('data/ETHUSDT', file))\n",
    "\n",
    "d = concat_parquet_files(paths)\n",
    "print('optimizing strategy ><><><')\n",
    "start = time.time()\n",
    "results = optimize_strategy(d.tail(int(len(d)/40)))\n",
    "print('done optimization')\n",
    "best_params, best_stats = find_best_parameters(results, metric='Sharpe Ratio')\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Stats:\")\n",
    "print(best_stats)\n",
    "\n",
    "\n",
    "\n",
    "print(f'total time is: {time.time()-sy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import vectorbt as vbt\n",
    "from src.utils import crossed_below, crossed_over\n",
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def backtest_strategy(data, tenkan_sen_length, kijun_sen_length, senkou_span_length):\n",
    "    # Step 1: Read the Parquet file\n",
    "    df = data.copy()\n",
    "\n",
    "    # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "\n",
    "    # # Resample to 15-minute intervals (adjust if needed)\n",
    "    # df_resampled = df.resample('1min', on='open_time').agg({\n",
    "    #     'open': 'first',\n",
    "    #     'high': 'max',\n",
    "    #     'low': 'min',\n",
    "    #     'close': 'last'\n",
    "    # })\n",
    "\n",
    "    # # Ensure all columns are properly aggregated\n",
    "    # df = df_resampled.reset_index()\n",
    "\n",
    "    # Ensure the column names are in lower case to match your data labels\n",
    "    df.columns = ['open_time', 'open', 'high', 'low', 'close']\n",
    "\n",
    "    # Step 2: Calculate Ichimoku Cloud components\n",
    "    # Calculate Ichimoku Cloud components with specified parameters\n",
    "    ichimoku_cloud, _ = ta.ichimoku(high=df['high'], low=df['low'], close=df['close'],\n",
    "                                    tenkan=tenkan_sen_length, kijun=kijun_sen_length, senkou=senkou_span_length,\n",
    "                                    include_chikou=False)\n",
    "\n",
    "    # Extract individual components\n",
    "    tenkan_sen = ichimoku_cloud[f'ITS_{tenkan_sen_length}']\n",
    "    kijun_sen = ichimoku_cloud[f'IKS_{kijun_sen_length}']\n",
    "    span_a = ichimoku_cloud[f'ISA_{tenkan_sen_length}']  # Adjust for span A\n",
    "    span_b = ichimoku_cloud[f'ISB_{kijun_sen_length}']\n",
    "\n",
    "    # Step 3: Define Strategy Logic\n",
    "    # Create signals DataFrame\n",
    "    signals = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Define Strategy Logic based on extracted components\n",
    "    # Long when tenkan_sen crosses over kijun_sen, close is above both span A and span B\n",
    "    long_entries = crossed_over(tenkan_sen, kijun_sen) & (df['close'] > span_a) & (df['close'] > span_b)\n",
    "\n",
    "    # Short when tenkan_sen crosses below kijun_sen, close is below both span A and span B\n",
    "    short_entries = crossed_below(tenkan_sen, kijun_sen) & (df['close'] < span_a) & (df['close'] < span_b)\n",
    "\n",
    "    # Exit conditions for long positions: tenkan_sen < kijun_sen\n",
    "    long_exits = crossed_below(tenkan_sen, kijun_sen)\n",
    "\n",
    "    # Exit conditions for short positions: tenkan_sen > kijun_sen\n",
    "    short_exits = crossed_over(tenkan_sen, kijun_sen)\n",
    "\n",
    "    # Assign strategy signals to the signals DataFrame\n",
    "    signals['long'] = long_entries.astype(int)\n",
    "    signals['short'] = short_entries.astype(int)\n",
    "    signals['long_exit'] = long_exits.astype(int)\n",
    "    signals['short_exit'] = short_exits.astype(int)\n",
    "\n",
    "    # Step 4: Backtest the Strategy with vectorbt\n",
    "    # Create portfolio\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        close=df['close'],\n",
    "        high=df['high'],\n",
    "        low=df['low'],\n",
    "        entries=signals['long'],\n",
    "        exits=signals['long_exit'],\n",
    "        short_entries=signals['short'],\n",
    "        short_exits=signals['short_exit'],\n",
    "        freq='1m',  # Frequency for closing positions (adjust as needed)\n",
    "        fees=0.001\n",
    "    )\n",
    "\n",
    "    # Calculate portfolio statistics\n",
    "    stats = portfolio.stats()\n",
    "\n",
    "    return stats\n",
    "\n",
    "def optimize_strategy_worker(params):\n",
    "    data, tenkan_sen_length, kijun_sen_length, senkou_span_length = params\n",
    "    print()\n",
    "    return (tenkan_sen_length, kijun_sen_length, senkou_span_length), backtest_strategy(data, tenkan_sen_length, kijun_sen_length, senkou_span_length)\n",
    "\n",
    "def optimize_strategy(data):\n",
    "    parameter_grid = {\n",
    "        'tenkan_sen_length': range(5, 51, 10),    # Adjust range and step size as needed\n",
    "        'kijun_sen_length': range(20, 61, 10),\n",
    "        'senkou_span_length': range(30, 91, 10)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    params_list = [(data, t, k, s) for t in parameter_grid['tenkan_sen_length']\n",
    "                   for k in parameter_grid['kijun_sen_length']\n",
    "                   for s in parameter_grid['senkou_span_length']]\n",
    "\n",
    "    # Use multiprocessing Pool to parallelize\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        for params, result in pool.imap_unordered(optimize_strategy_worker, params_list):\n",
    "            results[params] = result\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_scatter(results, metric='Sharpe Ratio'):\n",
    "    # Extract parameter values and metric values from results\n",
    "    tenkan_sen_values = [key[0] for key in results.keys()]\n",
    "    kijun_sen_values = [key[1] for key in results.keys()]\n",
    "    senkou_span_values = [key[2] for key in results.keys()]\n",
    "    \n",
    "    # Extract metric values\n",
    "    metric_values = []\n",
    "    for key in results.keys():\n",
    "        stats = results[key]\n",
    "        if metric == 'Sharpe Ratio':\n",
    "            metric_value = stats['Sharpe Ratio']\n",
    "        elif metric == 'Total Return':\n",
    "            metric_value = stats['Total Return']\n",
    "        else:\n",
    "            metric_value = None  # Handle additional metrics if needed\n",
    "        metric_values.append(metric_value)\n",
    "    \n",
    "    # Create scatter plot using Plotly\n",
    "    fig = go.Figure(data=go.Scatter3d(\n",
    "        x=tenkan_sen_values,\n",
    "        y=kijun_sen_values,\n",
    "        z=senkou_span_values,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=12,\n",
    "            color=metric_values,\n",
    "            colorscale='Viridis',  # Choose a colorscale\n",
    "            opacity=0.8,\n",
    "            colorbar=dict(title=metric)\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='Tenkan-sen Length',\n",
    "            yaxis_title='Kijun-sen Length',\n",
    "            zaxis_title='Senkou Span Length'\n",
    "        ),\n",
    "        title=f'Optimization Scatter Plot ({metric})'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "def plot_surface_with_contours(results, metric='Sharpe Ratio'):\n",
    "    # Extract parameter values and metric values from results\n",
    "    parameter_combinations = list(results.keys())\n",
    "    tenkan_sen_values = sorted(set([params[0] for params in parameter_combinations]))\n",
    "    kijun_sen_values = sorted(set([params[1] for params in parameter_combinations]))\n",
    "    senkou_span_values = [params[2] for params in parameter_combinations]\n",
    "    \n",
    "    # Extract metric values\n",
    "    metric_values = []\n",
    "    for params in parameter_combinations:\n",
    "        stats = results[params]\n",
    "        if metric == 'Sharpe Ratio':\n",
    "            metric_value = stats['Sharpe Ratio']\n",
    "        elif metric == 'Total Return [%]':\n",
    "            metric_value = stats.get('Total Return [%]', None)  # Use get() to handle missing keys gracefully\n",
    "        else:\n",
    "            metric_value = None  # Handle additional metrics if needed\n",
    "        metric_values.append(metric_value)\n",
    "    \n",
    "    # Create meshgrid for plotting\n",
    "    X, Y = np.meshgrid(tenkan_sen_values, kijun_sen_values)\n",
    "    \n",
    "    # Ensure Z is initialized with float type for NaNs\n",
    "    Z = np.zeros_like(X, dtype=np.float64)\n",
    "    \n",
    "    # Populate Z with metric values\n",
    "    for i, params in enumerate(parameter_combinations):\n",
    "        tenkan_index = tenkan_sen_values.index(params[0])\n",
    "        kijun_index = kijun_sen_values.index(params[1])\n",
    "        if metric_values[i] is not None:\n",
    "            Z[kijun_index, tenkan_index] = metric_values[i]\n",
    "    \n",
    "    # Create Surface plot using Plotly\n",
    "    fig = go.Figure(data=[go.Surface(\n",
    "        x=X,\n",
    "        y=Y,\n",
    "        z=Z,\n",
    "        contours_z=dict(\n",
    "            show=True,\n",
    "            usecolormap=True,\n",
    "            highlightcolor=\"limegreen\",\n",
    "            project_z=True,\n",
    "            highlightwidth=4,\n",
    "        ),\n",
    "        colorscale='Viridis'  # Choose a colorscale\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='Tenkan-sen Length',\n",
    "            yaxis_title='Kijun-sen Length',\n",
    "            zaxis_title='Metric Value' if metric else 'Senkou Span Length'\n",
    "        ),\n",
    "        title=f'Optimization Surface Plot ({metric})',\n",
    "        autosize=True,\n",
    "        width=800,\n",
    "        height=600,\n",
    "        margin=dict(l=65, r=50, b=65, t=90)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "def concat_parquet_files(file_paths):\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate through each file path\n",
    "    for file_path in file_paths:\n",
    "        # Read the parquet file\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "        df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the list along rows\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return concatenated_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Reading data...\")\n",
    "    paths = [os.path.join('data/ETHUSDT', file) for file in os.listdir('data/ETHUSDT') if file.endswith('.parquet')]\n",
    "    data = concat_parquet_files(paths)\n",
    "\n",
    "    print('Optimizing strategy...')\n",
    "    start = time.time()\n",
    "    results = optimize_strategy(data.tail(int(len(data)/50)))\n",
    "    print('Done optimization')\n",
    "\n",
    "    best_params, best_stats = max(results.items(), key=lambda x: x[1]['Total Return [%]'])\n",
    "\n",
    "    print('\\n')\n",
    "    print(\"---------------------\")\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Stats:\")\n",
    "    print(best_stats)\n",
    "\n",
    "    print(f'Total time taken: {time.time() - start} seconds')\n",
    "    print('\\n')\n",
    "    print(\"---------------------\")\n",
    "    plot_surface_with_contours(results, metric='Total Return')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import plotly.graph_objects as go\n",
    "import os \n",
    "\n",
    "def chandelier_exit_signals(df, length=22, mult=3.0, use_close=True):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Calculate ATR\n",
    "    df_copy['atr'] = ta.atr(df_copy['high'], df_copy['low'], df_copy['close'], length)\n",
    "\n",
    "    # Calculate Long Stop and Short Stop\n",
    "    if use_close:\n",
    "        df_copy['long_stop'] = df_copy['close'].rolling(length).max() - mult * df_copy['atr']\n",
    "        df_copy['short_stop'] = df_copy['close'].rolling(length).min() + mult * df_copy['atr']\n",
    "    else:\n",
    "        df_copy['long_stop'] = df_copy['high'].rolling(length).max() - mult * df_copy['atr']\n",
    "        df_copy['short_stop'] = df_copy['low'].rolling(length).min() + mult * df_copy['atr']\n",
    "\n",
    "    # Forward fill initial stop values\n",
    "    df_copy['long_stop'] = df_copy['long_stop'].ffill()\n",
    "    df_copy['short_stop'] = df_copy['short_stop'].ffill()\n",
    "\n",
    "    # Calculate directional logic\n",
    "    df_copy['dir'] = 1\n",
    "    df_copy.loc[df_copy['close'] < df_copy['long_stop'].shift(1), 'dir'] = -1\n",
    "    df_copy.loc[df_copy['close'] > df_copy['short_stop'].shift(1), 'dir'] = 1\n",
    "\n",
    "    # Generate buy and sell signals\n",
    "    df_copy['buy_signal'] = (df_copy['dir'] == 1) & (df_copy['dir'].shift(1) == -1)\n",
    "    df_copy['confirmed_buy_signal'] = df_copy['buy_signal'] & df_copy['dir'].shift(1).notna()\n",
    "\n",
    "    return df_copy['confirmed_buy_signal']\n",
    "\n",
    "def calculate_zlsma(df, length=50, offset=0, src='close'):\n",
    "    df_copy = df.copy()\n",
    "    src_series = df_copy[src]\n",
    "\n",
    "    # Calculate the first LSMA\n",
    "    df_copy['lsma'] = ta.linreg(src_series, length, offset)\n",
    "\n",
    "    # Calculate the second LSMA using the first LSMA\n",
    "    df_copy['lsma2'] = ta.linreg(df_copy['lsma'], length, offset)\n",
    "\n",
    "    # Calculate the Zero Lag LSMA\n",
    "    df_copy['eq'] = df_copy['lsma'] - df_copy['lsma2']\n",
    "    df_copy['zlsma'] = df_copy['lsma'] + df_copy['eq']\n",
    "\n",
    "    return df_copy['zlsma']\n",
    "\n",
    "def heikin_ashi(df):\n",
    "    column_names = df.columns.tolist()\n",
    "    heikin_ashi_df = ta.ha(open_=df['open'], high=df['high'], low=df['low'], close=df['close'])\n",
    "    heikin_ashi_df.columns = column_names[1:]\n",
    "    return heikin_ashi_df\n",
    "\n",
    "def entry_exit_signals(df, chandelier_signals, zlsma):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Entry signal: confirmed buy signal and current price is above ZLSMA\n",
    "    df_copy['entry_signal'] = chandelier_signals & (df_copy['close'] > zlsma)\n",
    "\n",
    "    # Exit signal: price closes below ZLSMA\n",
    "    df_copy['exit_signal'] = False  # Initialize exit signals column\n",
    "\n",
    "    # Track the state of being in a position\n",
    "    in_position = False\n",
    "\n",
    "    for i in range(1, len(df_copy)):\n",
    "        if df_copy.at[i, 'entry_signal']:  # If entry signal found\n",
    "            in_position = True  # Set position to true\n",
    "        elif in_position and (df_copy.at[i, 'close'] < zlsma.at[i]):  # If in position and exit condition\n",
    "            df_copy.at[i, 'exit_signal'] = True  # Mark exit signal\n",
    "            in_position = False  # Reset position\n",
    "\n",
    "    return df_copy[['entry_signal', 'exit_signal']]\n",
    "\n",
    "\n",
    "def plot_signals(df, chandelier_signals, zlsma, entry_exit_signals):\n",
    "    fig = go.Figure(data=[go.Candlestick(x=df.index,\n",
    "                                         open=df['open'],\n",
    "                                         high=df['high'],\n",
    "                                         low=df['low'],\n",
    "                                         close=df['close'],\n",
    "                                         name='Candlesticks')])\n",
    "\n",
    "    # Add Chandelier Exit buy signals\n",
    "    buy_signals = df[chandelier_signals]\n",
    "    fig.add_trace(go.Scatter(x=buy_signals.index,\n",
    "                             y=buy_signals['close'],\n",
    "                             mode='markers',\n",
    "                             marker=dict(color='green', size=10, symbol='triangle-up'),\n",
    "                             name='Chandelier Buy Signal'))\n",
    "\n",
    "    # Add ZLSMA\n",
    "    fig.add_trace(go.Scatter(x=df.index,\n",
    "                             y=zlsma,\n",
    "                             mode='lines',\n",
    "                             line=dict(color='white', width=2),\n",
    "                             name='Zero Lag LSMA'))\n",
    "\n",
    "    # Add entry signals\n",
    "    entry_signals = df[entry_exit_signals['entry_signal']]\n",
    "    fig.add_trace(go.Scatter(x=entry_signals.index,\n",
    "                             y=entry_signals['close'],\n",
    "                             mode='markers',\n",
    "                             marker=dict(color='blue', size=10, symbol='circle'),\n",
    "                             name='Entry Signal'))\n",
    "\n",
    "    # Add exit signals\n",
    "    exit_signals = df[entry_exit_signals['exit_signal']]\n",
    "    fig.add_trace(go.Scatter(x=exit_signals.index,\n",
    "                             y=exit_signals['close'],\n",
    "                             mode='markers',\n",
    "                             marker=dict(color='red', size=10, symbol='x'),\n",
    "                             name='Exit Signal'))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(title='Candlestick Chart with Buy/Sell Signals',\n",
    "                      yaxis_title='Price',\n",
    "                      xaxis_title='Date')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def create_portfolio(heikin_ashi_df, entry_exit_df, initial_cash=10000, slippage=0.001, fees=0.001):\n",
    "    # Initialize portfolio\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        close=heikin_ashi_df['close'],\n",
    "        entries=entry_exit_df['entry_signal'],\n",
    "        exits=entry_exit_df['exit_signal'],\n",
    "        size=1.0,  # Always invest 100% of capital\n",
    "        direction='longonly',  # Only go long\n",
    "        fees=fees,  # Transaction fees in percent\n",
    "        slippage=slippage,  # Slippage in percent\n",
    "        freq='5min'  # Assuming frequency of trading signals\n",
    "    )\n",
    "\n",
    "    return portfolio\n",
    "\n",
    "def concat_parquet_files(file_paths):\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate through each file path\n",
    "    for file_path in file_paths:\n",
    "        # Read the parquet file\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Convert 'open_time' to datetime assuming it's in milliseconds\n",
    "        df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the list along rows\n",
    "    concatenated_df = (pd.concat(dfs, ignore_index=True)).sort_values(by='open_time', ascending=True)\n",
    "    concatenated_df.reset_index(drop=True, inplace=True)\n",
    "    return concatenated_df\n",
    "\n",
    "path = 'data/BTCUSDT'\n",
    "print(\"Reading data...\")\n",
    "paths = [os.path.join(path, file) for file in os.listdir(path) if file.endswith('.parquet')]\n",
    "data = concat_parquet_files(paths)\n",
    "df_resampled = data.resample('5min', on='open_time').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    })\n",
    "heikin_ashi_df = heikin_ashi(df_resampled)\n",
    "chandelier_signals = chandelier_exit_signals(heikin_ashi_df)\n",
    "zlsma = calculate_zlsma(heikin_ashi_df)\n",
    "entry_exit_df = entry_exit_signals(heikin_ashi_df, chandelier_signals, zlsma)\n",
    "print(\"Backtesting...\")\n",
    "\n",
    "portfolio = create_portfolio(heikin_ashi_df, entry_exit_df)\n",
    "\n",
    "print(portfolio.stats())\n",
    "\n",
    "portfolio.plot().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
